<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <meta property="og:type" content="website">
<meta property="og:title" content="LZW&#39; Blog">
<meta property="og:url" content="liulin1995@github.io/index.html">
<meta property="og:site_name" content="LZW&#39; Blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LZW&#39; Blog">





  
  
  <link rel="canonical" href="liulin1995@github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>LZW' Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LZW' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Startseite</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archiv</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/梯度下降法、牛顿法和拟牛顿法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/梯度下降法、牛顿法和拟牛顿法/" class="post-title-link" itemprop="url">梯度下降法、牛顿法和拟牛顿法</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 15:00:24 / Geändert am: 15:01:09" itemprop="dateCreated datePublished" datetime="2019-07-13T15:00:24+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>梯度下降法是机器学习里面最常用的优化方法，简单直接效果还好，因此被广泛应用。今天就来回顾下梯度下降法。</p>
<p>梯度下降法有一个很trivial的想法，比如你在山上，你想要最快的下山，那么你就要走最陡的路下山，即坡度最大的路下山。反映在数学里面，山高为一个目标函数$f(x)$, 你当前的位置是$x_0 $, 当前坡度最大的路对应于当前位置$x$梯度最大的方向，但是由于我们是要下山，我们要反着方向走，即方向为</p>
<script type="math/tex; mode=display">
- \frac{\partial f}{\partial x}</script><p>确定了方向之后，我们需要确定要走的步长$\alpha$, 由于梯度是在$x_0 $处得到的，它只能够表示$f(x)$在点$x_0$出的陡峭程度，如果我们走的步伐太大，脱离的点$x_0$的邻域，那么这个梯度并不会是一个很好的下山方向。因此我们要保证这个步长$\alpha$足够小，此时，我们迈出了下山的第一步：</p>
<script type="math/tex; mode=display">
x_1 = x_0 - \alpha\frac{\partial f}{\partial x}</script><p>一次类推，只要我们一小步一小步的走陡峭的路，很快就能下山：</p>
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha\frac{\partial f}{\partial x}</script><p>下面用稍微严谨一点的数学说明下为什么要走负梯度的方向。假设我们在$f(x)$的$x_0 $处走一小步$\Delta x$，</p>
<script type="math/tex; mode=display">
f(x_0 + \Delta x)</script><p>我们的目标是走了这一步后的$f(x_0 + \Delta x)$相比$f(x_0)$下降的最多。首先，我们先在$x_0$处对$f(x)$进行一阶泰勒展开：</p>
<script type="math/tex; mode=display">
f(x)=f(x_0) + f'(x_0)\Delta x + o(f)</script><p>让$f(x)$下降的最多的点即为$x_1$,让要想让近似后的$f(x)$ 最小的话，就要让$f’(x_0)\Delta x$的值最小，考虑到两个向量的内积最小的问题，便要求$\Delta x$ 和梯度是反向的。实际上，只要$\Delta x$和梯度的夹角是大于90度的时候，上面的$argmin \ f(x)$就可以比$f(x_0)$小，当$\Delta x$和梯度的夹角是90度或者$\Delta x$是0的时候，$argmin  \ f(x)$和$f(x_0)$相等。此时令$f(x)$最小的$x$即为$x_{1}$,同时有$x_1 = x_0 + \Delta x$.</p>
<p>更一般的，考虑在$x_k$处对函数$f(x)$进行一阶泰勒展开，则能到的迭代更新公式为：</p>
<script type="math/tex; mode=display">
x_{k+1} = x_k-f'(x_k)</script><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>梯度下降法只是用到了一阶的导数信息，而牛顿法可以用到二阶的导数信息。</p>
<p>首先，我们先在$x_k$处对$f(x)$进行二阶泰勒展开,$\Delta x = x - x_k$：</p>
<script type="math/tex; mode=display">
f(x)=f(x_k) + f'(x_k)\Delta x + \frac{1}{2} f''(x_k)\Delta^2 x+ o(f)</script><p>让$f(x)$下降的最多的点即为$x_{k+1}$，即要$f(x)$取$f’(x)=0$的点，即让：</p>
<script type="math/tex; mode=display">
f'( x)=0</script><script type="math/tex; mode=display">
and \ \ f'( x) = f''(x_k)\Delta x + f'(x_k)</script><p>可得：</p>
<script type="math/tex; mode=display">
\Delta x = -f'(x_k)/f''(x_k) \\
so \\
x_{k+1} = x_k -f'(x_k)/f''(x_k)</script><p>假设$f’(x_k) = g, f’’(x_k)=H$为一个hessian矩阵，那么$\Delta x$的矩阵形式为：</p>
<script type="math/tex; mode=display">
\Delta x =-H^{-1}_k*g_k</script><p>此时$x$的更新公式为：</p>
<script type="math/tex; mode=display">
x_{k+1} = x_k - H^{-1}_{x_k}*g_{x_k}</script><h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>牛顿法的迭代中，需要计算hessian矩阵的逆，这个计算很耗时。而拟牛顿法考虑使用一个矩阵$G$来近似hessian矩阵$H_{x_k}^{-1}$ 。考虑何种的矩阵能够近似hessian矩阵。</p>
<p>按上面的等式(9)。取$x=x_{k+1}$,有：</p>
<script type="math/tex; mode=display">
g_{k+1}=H_k\Delta x + g_k</script><p>即</p>
<script type="math/tex; mode=display">
g_{k+1} - g_k =H_k\Delta x</script><script type="math/tex; mode=display">
 H_k^{-1}(g_{k+1}-g_k)=\Delta x</script><p>这个是hessian矩阵满足的条件，被称为拟牛顿条件。</p>
<p>如果$H_k$矩阵是正定的，那么可以保证牛顿法搜索是下降的，这个是因为根据（11）有</p>
<script type="math/tex; mode=display">
f(x) = f(x_k) - g_k^TH^{-1}_kg_k + \frac{1}{2}g_k^TH^{-1}_k HH^{-1}_kg_k \\
f(x) = f(x_k) - \frac{1}{2}g_k^TH^{-1}_kg_k</script><p>因为$H_k$矩阵是正定的，$H^{-1}_k$也会是正定的，那么有$g_k^TH^{-1}_kg_k$大于0，总有$f(x)&lt;f(x_k)$，即牛顿法是下降的。</p>
<p>拟牛顿法是牛顿法的近似，选择$G_{k}$作为$H^{-1}_k$的近似，要求矩阵$G_k$满足同样的条件。首先$G_k$是正定的，其次，$G_k$满足上面的拟牛顿条件，即</p>
<script type="math/tex; mode=display">
G_k(g_{k+1} - g_k )=\Delta x</script><p>这样子的方法被统称为拟牛顿法。同时可以使用下面的方式来更新$G_{k+1}$:</p>
<script type="math/tex; mode=display">
G_{k+1}= G_k + \Delta G</script><p>下面介绍BFP算法</p>
<p>假设每一步中的$G_{k+1}$是由$G_{k}$加上两个附加项构成的：</p>
<script type="math/tex; mode=display">
G_{k+1}=G_k + P_k + Q_k</script><p>要求：</p>
<script type="math/tex; mode=display">
G_{k+1}(g_{k+1} - g_k )=\Delta x \\
(G_k + P_k + Q_k)(g_{k+1} - g_k )=\Delta x</script><p>可以让$P_k (g_{k+1} - g_k )=\Delta x$且$Q_k(g_{k+1} - g_k )=-G_k(g_{k+1} - g_k )$ 使得更新后$G_{k+1}$满足拟牛顿条件。</p>
<p>事实上，这样子的满足条件的$P_k$和$Q_k$并不难找。例如取：</p>
<script type="math/tex; mode=display">
P_k=\frac{\Delta x^T\Delta x }{\Delta x ^T( g_{k+1} - g_k )}</script><script type="math/tex; mode=display">
Q_k = -\frac{G_k( g_{k+1} - g_k )( g_{k+1} - g_k )^TG_k}{( g_{k+1} - g_k )^TG_k( g_{k+1} - g_k )}</script><p>那么可以得到</p>
<script type="math/tex; mode=display">
G_{k+1}= G_k +　\frac{\Delta x^T\Delta x }{\Delta x ^T( g_{k+1} - g_k )}　-\frac{G_k( g_{k+1} - g_k )( g_{k+1} - g_k )^TG_k}{( g_{k+1} - g_k )^TG_k( g_{k+1} - g_k )}</script><p>BFGS算法是一个很常见的算法，用的是用一个正定矩阵$B_k$ 来近似$H_k$ ,同样需要满足上面说的拟牛顿条件。类似BFP算法，也有</p>
<script type="math/tex; mode=display">
B_{k+1}=B_k + P_k + Q_k</script><p>考虑使$P_k,Q_k$满足：</p>
<script type="math/tex; mode=display">
P_k\Delta x = (y_{k+1} - y_{k}) \\
Q_k \Delta x = -B_k\Delta x</script><p>找到满足条件的$P_k,Q_k$，得到BFGS的算法矩阵$B_{k+1}$ 的迭代公式：</p>
<script type="math/tex; mode=display">
B_{K+1}=B_k + \frac{( g_{k+1} - g_k )( g_{k+1} - g_k )^T}{( g_{k+1} - g_k )^T\Delta x} - \frac{B_k\Delta x \Delta x^T B_k}{\Delta^TB_k\Delta x}</script><p>实际的BFGS的算法中，有一个确定步长的步骤，即首先确定更新的方向$-B_k^{-1} g_k$, 然后求一个步长$\alpha $,求法如下：</p>
<script type="math/tex; mode=display">
f(x_k + \alpha_k (-B_k^{-1} g_k)) = min f(x_k + \alpha (-B_k^{-1} g_k)) \\
x_{k+1} = x_k+\alpha_k(-B_k^{-1} g_k)</script>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/operator/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/operator/" class="post-title-link" itemprop="url">soft-thresholding operator</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:58:42 / Geändert am: 14:59:17" itemprop="dateCreated datePublished" datetime="2019-07-13T14:58:42+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>让$f(x) = \lambda |x|_1$ ， 那么$f$的近似投影被定义为：</p>
<script type="math/tex; mode=display">
prox_f(x) = argmin_x\{\frac{1}{2}|x-z|_2^2 + \lambda|z|_1\}</script><p>该问题的最优性条件为：</p>
<script type="math/tex; mode=display">
0 \ \in \Delta(\frac{1}{2}|x-z|_2^2)+\partial (\lambda|z|_1)</script><p>由于$l1 \ norm$是可以分离的，因此我们可以单独考虑x中的每一个成分。</p>
<p>首先考虑$z_i \neq 0$的情况，此时有$\partial (\lambda|z|_1)=sign(z_i)$并且最优的$z_i^*$为：</p>
<script type="math/tex; mode=display">
0 = z_i - x_i + \lambda sign(z_i) \leftrightarrow 0 \in z - x + \lambda|x|_1</script><p>需要指出是，如果$z_i^<em> &lt;0$那么$x_i &lt;-\lambda$。同时如果$z_i^</em> &gt;0$那么$x_i &gt;\lambda$。因此，$|x_i| &gt;\lambda$ 和$sign(z_i^*) = sign(x_i)$.代入道先前的等式得到：</p>
<script type="math/tex; mode=display">
z_i^* = x_i - \lambda sign(x_i)</script><p>然后考虑$z_i = 0$的情况。此时$l_1 \ norm$的次微分为区间$[-1,1]$，并且最优性条件为：</p>
<script type="math/tex; mode=display">
0 = - x_i + \lambda [-1,1] \leftrightarrow x_i \in [-\lambda,\lambda] \leftrightarrow |x|_1 \le \lambda</script><p>将两个情况放在一起得到了：</p>
<script type="math/tex; mode=display">
[porx_f(x)]_i = z_i^* = \begin{cases}
    0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if |x_i| \le \lambda\\
    x_i - \lambda sign(x_i) \ \ \  if |x_i| >\lambda\\
\end{cases}</script><p>等式(6)同样可以写为：</p>
<script type="math/tex; mode=display">
[porx_f(x)]_i = sign(x_i)max(|x_i|-\lambda,0) \\
\ \ \ \ \ \ \ \ \ \ = sign(x_i)(|x_i| - \lambda)_+</script>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/Thresholding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/Thresholding/" class="post-title-link" itemprop="url">Singular Value Thresholding</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:57:40 / Geändert am: 14:59:44" itemprop="dateCreated datePublished" datetime="2019-07-13T14:57:40+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>SVT（奇异值阈值）奇异值收缩（singular value shrinkage)</p>
<p>考虑一个秩为r的矩阵$X\in R^{n1*n2}$的奇异值分解如下：</p>
<script type="math/tex; mode=display">
X= U \Sigma V^*，\Sigma = diag(\{\sigma_i\}_{1 \le i \le r})</script><p>其中$U,V$分别是$n1<em>r，n2</em>r$的正交矩阵，奇异值$\sigma_i$非负。</p>
<p>对于每一个$\tau \ge 0$，有软阈值操作$D_\tau$：</p>
<script type="math/tex; mode=display">
D_\tau(X) ：= UD_\tau(\Sigma)V^*, \ \ \ D_\tau(\Sigma)= diag(\{(\sigma_i - \tau)_+\}_{1 \le i \le r})</script><p>可以看出，这个软阈值操作是作用在奇异值上的，使他们趋于0，这也是这个被叫做奇异值收缩的原因。</p>
<p>对于奇异值收缩$D_\tau$ 有一个重要的结论：</p>
<script type="math/tex; mode=display">
D_\tau（Y)= arg\ min_X \frac{1}{2} |X-Y|_F^2 + \tau|X|_*</script>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/Softmax求导/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/Softmax求导/" class="post-title-link" itemprop="url">Softmax求导</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:56:31 / Geändert am: 14:57:08" itemprop="dateCreated datePublished" datetime="2019-07-13T14:56:31+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>softmax 可以看作是logistic “one versus all”的多分类版本，和普通的“one versus all”不同的是，softmax是通过选择最大的输出概率值来做最后的决策任务，因此不存在”混淆区域“</p>
<p>softmax的公式为：</p>
<script type="math/tex; mode=display">
p(Y=k|x)=\frac{exp(w_k * x)}{1+\Sigma_i^{K-1}{exp(w_i * x)}}  \\
p(Y=K|x)=\frac{1}{1+\Sigma_i^{K-1}{exp(w_i * x)}}</script><p>为什么$p(Y=K|x)=\frac{1}{1+\Sigma_i^{K-1}{exp(w_i <em> x)}}$？是因为，softmax是存在参数冗余的，即一开始所有的类别k都有一个$w_k$的，但是，如果对上面下面都除以一个$exp(w_K</em>x)$的话，就得到上面的公式：</p>
<script type="math/tex; mode=display">
p(Y=k|x)=\frac{exp(w_k * x)}{\Sigma_i^{K}{exp(w_i * x)}}  \\
上下都除以exp(w_K*x)得到： \\
p(Y=k|x)_{k\ne K}= \frac{exp((w_k-w_K)*x)}{\Sigma_i^{K}{exp((w_i-w_K) * x)}}=\frac{exp((w_k-w_K)*x)}{1+\Sigma_i^{K-1}{exp((w_i-w_K) * x)}}\\
p(Y=k|x)_{k = K}= \frac{exp((w_K-w_K)*x)}{\Sigma_i^{K}{(w_i-w_K) * x}}=\frac{1}{1+\Sigma_i^{K-1}{exp((w_i-w_K) * x)}}</script><p>令新的到的$w_i- w_K$为新的$w_i$即得到公式(1)。</p>
<p>得到softmax的参数的过程是通过<strong>最大似然函数</strong>得到的。</p>
<script type="math/tex; mode=display">
likelihood = \Pi_i^N\Pi_k^K1\{y_i=k\}p(Y=k|x)</script><p>其中$1\{y_i=k\}在$$\{\}$内表达式为真的时候取1。由于有多个连乘，做法是对其log，得到它的对数似然函数：</p>
<script type="math/tex; mode=display">
loglikelihood = \Sigma_i^N\Sigma_k^K \ log \ 1\{y_i=k\} \ p(Y=k|x)\\
=\Sigma_i^N\Sigma_k^K log \ \frac{1\{y_i=k\}*exp(w_k * x_i)}{1+\Sigma_j^{K-1}{exp(w_j * x_i)}}\\
=\Sigma_i^N\Sigma_k^K\{\{1\{y_i=k\}*w_k*x_i -log\{1+\Sigma_j^{K-1}{exp(w_j * x)}\}\}\}</script><p>此外，$log\{1+\Sigma_i^{K-1}{exp(w_i * x)}\}<script type="math/tex">对于</script>w_k$求导有：</p>
<script type="math/tex; mode=display">
\frac{\partial log\{1+\Sigma_j^{K-1}{exp(w_i * x_i)}}{\partial w_k}=\frac{1}{1+\Sigma_j^{K-1}{exp(w_j * x_i)}}*\frac{\partial (1+\Sigma_j^{K-1}{exp(w_j * x_i)})}{\partial w_k} \\
= \frac{1}{1+\Sigma_j^{K-1}{exp(w_j * x_i)}}*exp(w_k*x_i)*x=p(Y=k|x_i)*x_i</script><p>因此对对数似然函数求导为：</p>
<script type="math/tex; mode=display">
\frac{\partial loglikellihood}{\partial w_j}=\Sigma_i^N\Sigma_k^K\{1\{y_i=k\}w_k*x_i -log\{1+\Sigma_j^{K-1}{exp(w_j * x)}\}\} \\
= \Sigma_i^N \{1\{y_i=j\}x_i -x_i*p(Y=j|x_i)\}\}\\
=\Sigma_i^N x_i*(1\{y_i=j\} - p(Y=j|x_i))\}</script><p>可知，样本是否属于j类，样本i对于$w_j$的梯度贡献不同(这里假设是最小化log likelihood,需要对梯度取反）：</p>
<script type="math/tex; mode=display">
if \ x_i \in class_j \\
the \ contribution \ of \ x_i \ to \ \partial_{w_j} \ is: \ -x_i(1-p(y=j|x_i)) \\
else:\\
the \ contribution \ of \ x_i \ to \ \partial_{w_j} \ is: \ x_i*p(Y=j|x_i)</script><p>回顾到感知机的更新为用错误的样本对权重$w$进行更新，</p>
<script type="math/tex; mode=display">
W \leftarrow W - (-y_ix_i) \\
b \leftarrow W - (-y_i)</script><p>可以看到softmax除了对错误样本进行更新和感知机是吻合的，不同指出是softmax也用正确的样本来更新权重。</p>
<p>可以把$x_i(1-p(y=j|x_i))$ 重新写为：$-x_i(p(y=j|x_i) - 1)$</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/GBDT提升树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/GBDT提升树/" class="post-title-link" itemprop="url">GBDT提升树</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:54:36 / Geändert am: 14:55:13" itemprop="dateCreated datePublished" datetime="2019-07-13T14:54:36+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提升树是以分类树或者回归树为基本分类器的提升方法。提升方法采用的加法模型和前向分布算法。提升树模型可以表示为决策树的加法模型：</p>
<script type="math/tex; mode=display">
f_M(x) = \Sigma_{m=1}^{M} T(x;W_m)</script><p>梯度提升树采用的是前向分布算法。第m步骤的模型为：</p>
<script type="math/tex; mode=display">
f_m(x;W_m)=f_{m-1} + T(x;W_m)</script><p>则，通过经验风险极小化来确定下一棵数的参数$W_m$, </p>
<script type="math/tex; mode=display">
\hat{W}_m  = argmin \ \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)+T(x_n;W_m))</script><h2 id="一阶的GBDT"><a href="#一阶的GBDT" class="headerlink" title="一阶的GBDT"></a>一阶的GBDT</h2><p>考虑一阶泰勒展开，对$obj(m)= \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)+T(x_n;W_m)$在$f_{m-1}(x)$处展开有：</p>
<script type="math/tex; mode=display">
obj(m) = \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)+T(x_n;W_m) \thickapprox  \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)) + \frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }*T(x_n;W_m) + \varTheta (f(x_n))</script><p>对近似一阶展开最小化，可知：$L(y_n,f_{m-1}(x_n))$,$\frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }$都是constant，因为为了最小化(4)，只要求：</p>
<script type="math/tex; mode=display">
T(x_n;W_m) = -1*\frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }</script><p>这个就得到了梯度提升树的算法，新的一颗树是通过拟合负梯度得到的。</p>
<p>需要注意的是，当$L(y,f_m(x))$选为平方函数的时候，上述的负梯度即残差。</p>
<h2 id="二阶的GBDT"><a href="#二阶的GBDT" class="headerlink" title="二阶的GBDT"></a>二阶的GBDT</h2><p>考虑二阶展开，有：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)) + \frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }*T(x_n;W_m)+ \frac{1}{2} \frac{\partial L(y_n,f(x_n))}{\partial^2f _{m-1}(x_n) }*T^2(x_n;W_m) +\varTheta (f(x_n))</script><p>可以缩写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N   L(y_n,f_{m-1}(x_n)) + g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)</script><p>由于函数中的常量在优化过程中不起影响，因此可以省略，(7)可以进一步写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N    g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)</script><p>在更一般的机器学习的目标函数中，会通过添加一个正则项，来达到一个最小化结构化误差的作用，因此(8)进一步写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N    g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)+\Omega(T(x;W_m))</script><p>最常见的一个二阶的GBDT模型为Xgboost模型。</p>
<h2 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h2><p>这里假设一棵决策树，叶子节点个数为$M$,该决策树是由叶子节点上的值组成的向量$\omega \in R^M$，以及一个将一个特征向量映射到叶子节点的索引函数$q:R^d \rightarrow \{1,2,…,M\}$ 组成的。因此一个决策树可以重新写为：$T(x)=\omega_{q(x)}$</p>
<p>此时正则项定义为：$\Omega(T) = \gamma M + \frac{1}{2}<em> \lambda</em> \Sigma_{j=1}^M\omega^2$来定义。此时为叶子j定义一个样本集合为：$I_j = \{n|q(x_n)=j\}$</p>
<p>上面的目标函数可以重新写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N    g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)+\Omega(T(x;W_m)) \\
obj(m)\thickapprox   \Sigma_{n=1}^N    g_n*\omega_{q(x_n)} + \frac{1}{2}*h_n*\omega^2_{q(x_n)}+\gamma M + \frac{1}{2}* \lambda* \Sigma_{j=1}^M\omega^2 \\
obj(m)\thickapprox   \Sigma_{j=1}^M  (\Sigma_{n \in I_j}g_n)  *\omega_j + \frac{1}{2}*(\Sigma_{n \in I_j}h_n+\lambda)*\omega^2_{j}+\gamma M</script><p>定义$G_n = \Sigma_{n \in I_j}g_n,H_n=(\Sigma_{n \in I_j}h_n)$,那么上面的目标函数可以写为：</p>
<script type="math/tex; mode=display">
obj(m)\thickapprox   \Sigma_{j=1}^M  G_n *\omega_j + \frac{1}{2}*(H_n+\lambda)*\omega^2_{j}+\gamma M</script><p>对于一个单变量二次方程式:</p>
<script type="math/tex; mode=display">
argmin_x Gx+\frac{1}{2}Hx^2 = - \frac{G}{H}, H>0\\
min_x Gx+\frac{1}{2}Hx^2 = - \frac{G^2}{H}</script><p> 假设树的结构是固定的，那么每个叶子的最佳权重为：</p>
<script type="math/tex; mode=display">
\omega_j^* = -\frac{G_j}{H_j+\lambda}</script><script type="math/tex; mode=display">
obj = -\frac{1}{2}\Sigma_{j=1}^M \frac{G_j^2}{H_j + \lambda} + \gamma M</script><p>如果给定一个树的结果，可以用上面的公式计算出得分，并且为每个叶子赋值。但是问题是，如何找到一个合适的结构。</p>
<p>xgb中的方法是贪婪的</p>
<ul>
<li><p>树从深度0开始</p>
</li>
<li><p>对树的每一个叶子节点，尝试增加一个划分。目标函数的变化程度为：</p>
<script type="math/tex; mode=display">
Gain= \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma</script></li>
</ul>
<p>对于每一个节点，遍历所有的特征：</p>
<ul>
<li>对于每一个特征，根据特征值对样本进行排序</li>
<li>使用线性扫描的方法去决定特征的最佳划分</li>
<li>选择所有特征中最佳的特征</li>
</ul>
<p>回到GBDT，生成的树要加到原来的模型上：</p>
<script type="math/tex; mode=display">
f_m(x;W_m)=f_{m-1} + \epsilon T(x;W_m)</script><p>这里$\epsilon$被称为步长或者收缩，一般设置为0.1左右。这个意味着在每一步没有做最好的优化，保留了后面的轮数，有助于防止过拟合。</p>
<p>除了步长外，xgb还有其他的防止过拟合的措施：</p>
<ul>
<li><p>Early Stopping：本质是在某项指标达标后就停止训练，也就是设定了训练的轮数</p>
</li>
<li><p>Subsampling：无放回抽样，具体含义是每轮训练随机使用部分训练样本，其实这里是借鉴了随机森林的思想</p>
</li>
<li><p>colsample_bytree: 训练的过程中，使用的特征以一定的比例从所有的特征中采样。</p>
</li>
<li><p>max_depth: 树的深度，树越深越容易过拟合</p>
</li>
<li><p>min_child_weight: 值越大，越容易欠拟合。</p>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>  [李航《统计学习方法》]: </p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/拉格朗日对偶性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/拉格朗日对偶性/" class="post-title-link" itemprop="url">拉格朗日对偶性</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:53:25 / Geändert am: 14:53:55" itemprop="dateCreated datePublished" datetime="2019-07-13T14:53:25+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>拉格朗日对偶是约束最优化问题中最常见的一种方法，将带约束的问题转化为不带约束的问题，并进行求解。除此之外，拉格朗日对偶性有这非常好的性质，例如：</p>
<ul>
<li>对偶问题可以给出原问题的一个下界</li>
<li>无论原问题是否是凸的，对偶问题都是凸的</li>
<li>当满足一定的条件的时候，原始问题和对偶问题完全等价。</li>
</ul>
<h3 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h3><p>首先我们先提到原始问题。考虑这么一个带约束的问题：</p>
<script type="math/tex; mode=display">
min_{x} f(x)\\
s.t \ \ c_i(x) \le 0,\ i=1,2,...,k \\
\ \ \ \  h_j(x) = 0, \ j=1,2,...,m</script><p>同时引入广义拉格朗日函数 ：</p>
<script type="math/tex; mode=display">
L(x,\alpha,\beta) = f(x)+ \sum_{i=1}^k{\alpha_i c_i(x)} + \sum_{j=1}^m{\beta_j h_j(x)}</script><p>其中$\alpha_i$,$\beta_i$是拉格朗日乘子，$\alpha_i\ge0$。那么考虑以下的函数：</p>
<script type="math/tex; mode=display">
\theta_p(x) = max_{\alpha,\beta;\alpha\ge0}L(x,\alpha,\beta)</script><p>假设存在一个$x$,$x$会是以下四种情况之一：</p>
<ol>
<li>$x$满足约束$c_i(x)\le0,h_j(x)=0$</li>
<li>$x$不满足约束$c_i(x) \le 0 $</li>
<li>$x$不满足约束$h_j(x)=0$</li>
<li>$x$不满足约束$h_j(x)=0$和$h_j(x)=0$</li>
</ol>
<p>考虑第一种情况，此时$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\alpha_i = 0$,且$\beta_i$值任意，此时$\theta_p(x)$等价于$f(x)$.即此时$\theta_p(x)$等价于原问题。</p>
<p>考虑第二种情况，此时$c_i(x) &gt; 0$.$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\alpha_i \rightarrow +\infty$,且$\beta_i$值任意，此时$\theta_p(x) \rightarrow +\infty$。</p>
<p>考虑第三种情况，此时$h_j(x) \ne 0$,$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\beta_i \rightarrow +\infty或-\infty$,且$\alpha_i$值为0.此时，$\theta_p(x) \rightarrow +\infty$。</p>
<p>考虑第四种情况，此时$h_j(x) \ne 0$，$c_i(x) &gt; 0$,$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\beta_i \rightarrow +\infty或-\infty$,且$\alpha_i \rightarrow +\infty$，$\theta_p(x) \rightarrow +\infty$。</p>
<p>因此可以看出$\theta_p(x)$的性质，</p>
<script type="math/tex; mode=display">
\theta _p=\begin{cases}
    f\left( x \right)\\
    +\infty\\
\end{cases}</script><p>如果考虑问题</p>
<script type="math/tex; mode=display">
min_x  \theta_p(x)</script><p>该问题和原问题是等价的，即它们有等价的解或者同样无解。这样子就得到了广义拉格朗日函数的极小极大问题。定义原问题的最优解为：</p>
<script type="math/tex; mode=display">
p^* = min_x  \theta_p(x)</script><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>定义问题：</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta)=min_x L(x,\alpha,\beta)</script><p>再考虑极大化$\theta_D(\alpha,\beta)=min_x=L(x,\alpha,\beta)$,即</p>
<script type="math/tex; mode=display">
max_{\alpha_i,\beta_j;\alpha_i\ge 0} \theta_D(\alpha,\beta) = max_{\alpha_i,\beta_j;\alpha_i\ge 0} min_x L(x,\alpha,\beta)</script><p>该问题被称为广义拉格朗日函数的极大极小问题。该问题表达为带约束的优化问题</p>
<script type="math/tex; mode=display">
max_{\alpha_i,\beta_j} min_x L(x,\alpha,\beta) \\
s.t \ \ \alpha_i\ge 0, \ \ i=1,2,...,k</script><p>该带约束的问题被称为原始问题的对偶问题。同时定义该对偶问题的最优解为：</p>
<script type="math/tex; mode=display">
d^* = max_{\alpha_i,\beta_j} \theta_D(\alpha,\beta)</script><h3 id="对偶问题和原始问题的关系"><a href="#对偶问题和原始问题的关系" class="headerlink" title="对偶问题和原始问题的关系"></a>对偶问题和原始问题的关系</h3><h4 id="弱对偶性和强对偶性"><a href="#弱对偶性和强对偶性" class="headerlink" title="弱对偶性和强对偶性"></a>弱对偶性和强对偶性</h4><p>对偶问题和原始问题的最优解满足以下关系：</p>
<script type="math/tex; mode=display">
d^* \le p^*</script><p>证明很简单，即：</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta)=min_x L(x,\alpha,\beta) \le L(x,\alpha,\beta) \le max_{\alpha,\beta;\alpha\ge0}L(x,\alpha,\beta)=\theta_p(x)</script><p>即</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta) \le \theta_p(x)</script><p>则有</p>
<script type="math/tex; mode=display">
max_{\alpha,\beta} \  \theta_D(\alpha,\beta) \le min_x \ \theta_p(x) \\
d^* \le p^*</script><p>这个性质也被称为<strong>弱对偶性</strong>，对所有的优化问题成立，即使原始问题非凸。相对于弱对偶性，也有<strong>强对偶性</strong>：</p>
<script type="math/tex; mode=display">
d^* = p^*</script><h4 id="Slater条件"><a href="#Slater条件" class="headerlink" title="Slater条件"></a>Slater条件</h4><p>考虑原始问题和对偶问题。假设函数$f(x) , c_i(x)$是凸函数，并且$h_j(x)$是仿射函数，并且不等式$c_i(x)$是严格可行的,即存在x,对于所有的i有$c_i(x) &lt; 0$则存在$x^<em>,\alpha^</em>,\beta^<em>$,使得$x^</em>$是原问题的最优解，$\alpha^<em>,\beta^</em>$是对偶问题的最优解，同时有：</p>
<script type="math/tex; mode=display">
d^* = p^* = L(x^*,\alpha^*,\beta^*)</script><p><strong>Note</strong>, 该slater条件对应SVM即要求数据集是线性可分的；如果数据是线性不可分的，那么此时使用SVM寻找分隔超平面也失去了意义。</p>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>对于原始问题和对偶问题，假设假设函数$f(x) , c_i(x)$是凸函数，并且$h_j(x)$是仿射函数，并且不等式$c_i(x)$是严格可行的，则$x^<em>$是原问题的最优解，$\alpha^</em>,\beta^<em>$是对偶问题的最优解的充分必要条件是$x^</em>$$\alpha^<em>,\beta^</em>$满足以下的KKT条件：</p>
<script type="math/tex; mode=display">
\begin{cases} \Delta_x L(x^*,\alpha^*,\beta^*) =0
    \\ \Delta_\alpha L(x^*,\alpha^*,\beta^*) =0
    \\ \Delta_\beta L(x^*,\alpha^*,\beta^*) =0
     \\ \alpha_i^*c_i(x)=0, i=1,2,...,k 
     \\ c_i(x^*) \le 0 , i=1,2,...,k 
     \\ \alpha_i \ge 0 , i=1,2,...,k 
     \\ h_j(x^*)=0, j=1,2,...,m
\end{cases}</script><p>其中，$\alpha_i^<em>c_i(x)=0, i=1,2,…,k $被称为是KKT条件的对偶互补条件。由此条件可知：若$\alpha_i^</em> &gt;0$,则$c_i(x^*)=0$.</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>当原始问题为凸优化问题的时候，其对偶问题的强对偶性与KKT条件是互为充要的。</p>
<p>当原始问题不为凸优化问题是，利用其对偶问题也可以得到原问题最优解的下界。</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://blog.csdn.net/qq_32742009/article/details/81413068" target="_blank" rel="noopener">https://blog.csdn.net/qq_32742009/article/details/81413068</a></p>
<p>统计学习方法附录C</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/基于梯度的优化算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/基于梯度的优化算法/" class="post-title-link" itemprop="url">基于梯度的优化算法</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:51:43 / Geändert am: 14:52:41" itemprop="dateCreated datePublished" datetime="2019-07-13T14:51:43+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
                  . 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h4><p>使用整个训练集来优化权重，意思是用训练集中的所有样本的loss来更新模型的权重。此时，权重的梯度为：</p>
<script type="math/tex; mode=display">
\partial W = \frac{1}{N} \partial\Sigma_i^N \ L(f(x^{i}),y^{i}) \\
 =  \frac{1}{N} \Sigma_i^N \partial\ L(f(x^{i}),y^{i})</script><p>可以看出权重的梯度是每个每个样本的梯度的期望。</p>
<p>此时的更新过程为：</p>
<script type="math/tex; mode=display">
W := W -\eta \frac{1}{N} \Sigma_i^N \partial \ L(f(x^{i}),y^{i})</script><p>如果样本的个数的是10000， 100000个的时候，需要先遍历所有的样本在对权重$W$更新，这个是非常耗时的。因此有人提出了SGD</p>
<h4 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h4><p>相比与batch gradient descent方法， SGD不适用全部的样本来估计权重的梯度，而是使用的一小部分的样本。主要的原因是一小部分的样本的梯度均值可以近似真实的梯度，而这部分样本的数量越多，估计的梯度越近似真实的梯度。另一方面，用小部分样本来估计的梯度是带有噪声的，在一定程度也起到了正则化的作用。</p>
<p>此时的梯度为：</p>
<script type="math/tex; mode=display">
\partial W = \frac{1}{N} \partial\Sigma_i^m \ L(f(x^{i}),y^{i})</script><p>此时的更新过程为：</p>
<script type="math/tex; mode=display">
W := W -\eta \frac{1}{N} \Sigma_i^m \partial \ L(f(x^{i}),y^{i})</script><p>实际过程中，每次选取一部分的样本来估计梯度，并进行更新，这部分被叫做一个迭代；如果一整个的训练集都使用一次，就叫经过了一个 epoch。注意，每个epoch，样本都要被打散。</p>
<h4 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD+Momentum"></a>SGD+Momentum</h4><p>SGD方法存在这一个“震荡”的问题，如下图：</p>
<p><img src="E:\刘志伟的文件\论文笔记\机器学习\assets\1562846876860.png" alt="1562846876860"></p>
<p>可以看到在更新的过程中，梯度下降的迭方向会偏离实际中真正的下降方向。momentum就是为了克服</p>
<p>这个震荡的问题。</p>
<script type="math/tex; mode=display">
v_{t} = \eta v_{t-1} + (1-\eta)\partial W_t \\
W_t \ := W_{t-1}-\eta v_{t}</script><p>动量可以看作是迭代过程中下降方向，震荡的方向被消除了，使得权重更新的方向变得一致。</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>梯度下降中存在一个问题，如果使用相同的学习率，对于权重W中每一个元素$W_i$,如果他们的梯度值有较大的差别的时候，会导致权重在梯度值较小的维度上迭代过慢。Adagrad方法根据权重在每个维度上梯度值的大小来调整各个维度上的学习率，来避免统一学习率难以适应所有维度的问题。</p>
<script type="math/tex; mode=display">
s_t = s_{t-1} + \partial W_t \odot \partial W_t \\
W_t \ := W_{t-1}-\frac{\eta}{\sqrt{s_t}}\partial W_t</script><p>Adagrad存在一个问题，有与$s_t$一致累加着梯度按元素平方的和，因此如果权重的某个元素的偏导数一直较大，会使得学习率下降较快；反之，如果某个元素的偏导数一直较小，学习率则会下降较慢。所以Adagrad在后期的时候，比较难找到一个合适的解。</p>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>由于Adagrad存在后期学习率低的问题，RMSprop对其做了一点小小的改动：</p>
<script type="math/tex; mode=display">
s_t =  \gamma s_{t-1} + (1-\gamma) \ \partial W_t \odot \partial W_t \\
W_t \ := W_{t-1}-\frac{\eta}{\sqrt{s_t}}\partial W_t</script><p>RMSprop对这些梯度的平方做指数加权移动平均，使得每个权重的学习率不会一直降低或者不变。</p>
<p>可以消除梯度下降过程中的摆动，包括mini-batch，允许使用更大的$\alpha$,加快算法学习速度</p>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>Adadelta也针对Adagrad做了改进，同样的，Adadelta也是了梯度平方的加权移动平均。但是它与RMSprop不同的是Adadelta还维护了一个额外的状态变量$\Delta W_t$ 来计算自变量的变化值：</p>
<script type="math/tex; mode=display">
W_t^/ = \sqrt{\frac{\Delta W_{t-1} + \epsilon}{s_t + \epsilon}} \odot W_t</script><p>然后更新权重：</p>
<script type="math/tex; mode=display">
W_t = W_{t-1} -W_t^/</script><p>最后使用$\Delta W_t$来记录权重变化量的指数加权移动平均：</p>
<script type="math/tex; mode=display">
\Delta W_t = \rho\Delta W_{t-1} + (1-\rho)W_t^/  \odot W_t^/</script><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adam算法是将momentum和RMSprop结合起来</p>
<script type="math/tex; mode=display">
v_{t} = \beta_1 v_{t-1} + (1-\beta_1)\partial W_t \\
s_{t} = \beta_2 s_{t-1} +(1-\beta_2)\partial W_t \\
\hat{v}_{t} = \frac{v_{t}}{1-\beta_1^t}\\
\hat{s}_{t} = \frac{S_{t}}{1-\beta_2^t} \\
W_t:= W_{t-1}- \frac{\alpha \hat{v}_{t}}{\sqrt{\hat{s}_{t}+\epsilon}}</script><p>$\beta_1$常用为0.9，$\beta_2$常用为0.999，而$\epsilon$为10^-8。Adam还对动量项和 梯度平方项做了偏差修正，然后使用修正后的变量对权重进行更新。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/11/KL散度和交叉熵CE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/11/KL散度和交叉熵CE/" class="post-title-link" itemprop="url">KL散度和交叉熵CE</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-11 15:04:13 / Geändert am: 15:53:24" itemprop="dateCreated datePublished" datetime="2019-07-11T15:04:13+08:00">2019-07-11</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><p>首先回顾熵的定义：</p>
<script type="math/tex; mode=display">
H(x)=E_{x \sim  p}[I(x)]</script><p>其中$I[x]$是时间x=x的自信息：</p>
<script type="math/tex; mode=display">
I[x]=-log \ p(x)</script><p>自信息只处理单个输出，但是熵可以对整个分布的不确定性信息总量进行量化。</p>
<p>如果对于同一个随机变量x有两个单独的概率分布$P(x)$和$Q(x)$，可以使用KL散度来衡量这两个分布的差异。</p>
<script type="math/tex; mode=display">
D_{KL}(P||Q)= E_{x \sim  p}[log \frac{P(x)}{Q(x)}]=E_{x \sim  p}[log \ P(x) - log \ Q(x)]</script><p>需要注意的是$D_{KL}(P||Q)$不等于$D_{KL}(Q||P)$，KL散度并不是一个距离，因此他们有对称性。</p>
<p>KL散度的离散情况为：</p>
<script type="math/tex; mode=display">
D_{KL}(P||Q)= \Sigma_x\ P(x) [log\frac{P(x)}{Q(x)}] \\
= \Sigma_x\ P(x) [log\ P(x)-log \ Q(x)] \\
= \Sigma_x\ P(x) \ log\ P(x) - \Sigma_x\ P(x)\ log \ Q(x) \\</script><p>可以看的出来，KL散度表达的是一种编码在另一种编码表示下，所需要增加的熵的信息。</p>
<h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>交叉熵的定义为：</p>
<script type="math/tex; mode=display">
CE(P,Q)=-E_{x \sim  p}[log \ Q(x)]</script><p>离散情况为：</p>
<script type="math/tex; mode=display">
CE(P,Q) = \Sigma_x\ P(x)logQ(x)</script><p>它和KL散度很相似，二者只相差了左边的一项：</p>
<script type="math/tex; mode=display">
CE(P,Q) = H(P) + D_{KL}(P||Q)</script><p>需要注意的是，在机器学习中CE常被用作分类任务的loss函数。此时，由于样本的标签是固定的，则$ H(P)$的值是固定的，那么最小化CE就等价于最小化KL散度。这里的一个思想是吧样本的标签看作一个分布，样本的预测标签看成另一个分布。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/02/Cyclical-Learning-Rates-for-Training-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/02/Cyclical-Learning-Rates-for-Training-Neural-Networks/" class="post-title-link" itemprop="url">Cyclical Learning Rates for Training Neural Networks</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-02 22:12:09 / Geändert am: 22:15:53" itemprop="dateCreated datePublished" datetime="2019-07-02T22:12:09+08:00">2019-07-02</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="paper-details"><a href="#paper-details" class="headerlink" title="paper details"></a>paper details</h2><p>深度学习中有一个常识是，学习率在训练的过程中需要逐渐减小。但是这篇文章却给出了一个让人惊讶的事实，就是训练过程中的学习率如果是多变（rise and fall）的是有益于训练的。因此作者建议学习率在一个范围内周期变化，而不是将其设置为固定值。</p>
<p>Cyclical Learning Rates来源于这么一个观察：学习率的增加虽然会带来短期的副作用但是长期来看是有益的。因此这种观察引出了让学习率在一定范围内变化而不是采用逐步固定或指数递减值的想法。即设置一个最大和最小的边界，然后学习率在里面循环变化。如下图的 triangular learning rate policy：</p>
<p><img src="/images/assets/1557733855040.png" alt="1557733855040"></p>
<p>CLR能够发挥作用的一个直观理解是：最小化loss的困难在于如何逃离saddle点而不是在于差的局部最小值。在saddle 点的附近，梯度都很小因此学习的过程缓慢，因此通过增加学习率可以更快地走出saddle点区域。经验上的理由为什么CLR能够work是这样的：最佳的学习率可能在min-max boundaries之间，在最佳的学习率附近会被用于进行训练。（其他会被用于脱离saddle点。。。）。</p>
<p>除了上面显示的trangular learning rate policy,还有以下两种:</p>
<ol>
<li><p>triangular2, 和triangular差不多，差别在于每一个cycle之后lr会减半。</p>
</li>
<li><p>exp_range，boundary的值会以一个指数因子衰减。</p>
</li>
</ol>
<p><img src="/images/assets/1557737380100.png" alt="1557737380100"></p>
<p><img src="/images/assets/1557737358780.png" alt="1557737358780"></p>
<p>里面还讲了如何去估计一个cycle len的方法：</p>
<p>stepsize最好是2-10倍的每个epoch的迭代次数。对于CIFAR10来说，stepsize=8也就比stepsize=2效果好上一点点。</p>
<p>此外还讲了如何估计一个合理的min和max boundary</p>
<p>第一个方法就是：“LR range test”,模型先跑几个epoch，然后让lr从一个很小值增加到很大的值。然后画出accuracy versus learning rate.如下图：</p>
<p><img src="/images/assets/1557735295110.png" alt="1557735295110"></p>
<p>注意图中的accuracy开始增加和accuracy开始变缓的时间段(或者accuracy开始下降)的地方。 这两个地方是bound是的一个好的选择。即base_lr是第一个值，而max_lr是第二个值。或者说可以用一个经验，将base_lr设置为1/3或1/4的max_lr. <strong>论文中作者选了base_lr = 0.001,而max_lr = 0.006</strong></p>
<p>另外一个选择bound是的方法式画出loss versus learning rate的图，如下：</p>
<p><img src="/images/assets/1557735640796.png" alt="1557735640796"></p>
<p>这张图中最适合的lr是哪里？不是在最低点，因为在最低点的lr已经有点大了。我们需要的是一个点更aggressive，所以我们能够train很快。即那个点loss下降是<strong>最快</strong>的</p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>做kaggle比赛的时候，clr的base_lr和max_lr设置反了，特别是在开始的一个stepsize里面，速度非常快，很容易就达到了一个很好的acc ，但是过了这个stepsize，acc就不断下降。一开始举得clclr的问题，后来突然发现是我输入的参数错误。有鉴于它收敛非常快，我觉得还是要借鉴下，发现lr的变化是这样的：</p>
<p><img src="/images/assets/1557732898573.png" alt="1557732898573"></p>
<p>和clr差了一个stepsize。这个和SGDR很相似，准备用这个试试。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a></p>
<p><a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate" target="_blank" rel="noopener">https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate</a></p>
<p><a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0" target="_blank" rel="noopener">https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0</a></p>
<p><a href="https://www.paperweekly.site/papers/notes/598" target="_blank" rel="noopener">https://www.paperweekly.site/papers/notes/598</a></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/02/TorchVision-Image-Transforms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/02/TorchVision-Image-Transforms/" class="post-title-link" itemprop="url">TorchVison Image Transforms</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-02 22:09:59 / Geändert am: 22:10:49" itemprop="dateCreated datePublished" datetime="2019-07-02T22:09:59+08:00">2019-07-02</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>transforms主要是图像transform, 它们可以通过使用Compose来链接起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transforms.Compose([</span><br><span class="line">  transforms.CenterCrop(<span class="number">10</span>),</span><br><span class="line">  transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h2 id="Transforms-on-PIL-Image"><a href="#Transforms-on-PIL-Image" class="headerlink" title="Transforms on PIL Image"></a>Transforms on PIL Image</h2><h4 id="torchvision-transforms-CenterCrop-size"><a href="#torchvision-transforms-CenterCrop-size" class="headerlink" title="torchvision.transforms.CenterCrop(size):"></a><code>torchvision.transforms.CenterCrop</code>(<em>size</em>):</h4><p>对给定的PIL image在中心处裁剪。</p>
<p>参数为：<strong>size</strong>, int or sequence. 如果是一个sequence，比如（h,w）会裁剪一个h*w大小的图片。</p>
<p>如果是int，那么会裁剪大小为（size，size）的图像</p>
<h4 id="torchvision-transforms-FiveCrop-size"><a href="#torchvision-transforms-FiveCrop-size" class="headerlink" title="torchvision.transforms.FiveCrop(size)"></a><code>torchvision.transforms.FiveCrop</code>(<em>size</em>)</h4><p>对给定的PIL image的四个角和中心进行裁剪</p>
<p>其他同上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>transform = Compose([</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>   FiveCrop(size), <span class="comment"># this is a list of PIL Images</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>   Lambda(<span class="keyword">lambda</span> crops: torch.stack([ToTensor()(crop) <span class="keyword">for</span> crop <span class="keyword">in</span> crops])) <span class="comment"># returns a 4D tensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>])</span><br></pre></td></tr></table></figure>
<h4 id="torchvision-transforms-Pad-padding-fill-0-padding-mode-’constant’"><a href="#torchvision-transforms-Pad-padding-fill-0-padding-mode-’constant’" class="headerlink" title="torchvision.transforms.Pad(padding, fill=0, padding_mode=’constant’)"></a><code>torchvision.transforms.Pad</code>(<em>padding</em>, <em>fill=0</em>, <em>padding_mode=’constant’</em>)</h4><p>用给定的pad值对图像的4个sides进行填充</p>
<p>参数：padding: 用于确定每个border填充的数量. </p>
<p>​    如果只有一个int，对所有的边进行一样的填充数量</p>
<p>​    如果为长度为2的tuple，那么是对左右，上下分别指定</p>
<p>​    如果长度为4的tuple，那么是对左、上，右、下的边分别指定 </p>
<p>fill: 当mode为constfill时的填充值。默认为0，如果是一个长度为3的tuple是，分别为RGB值</p>
<p>padding_mode:padding的类型</p>
<p>​    constant，常数填充</p>
<p>​    edge：用edge上的值进行填充</p>
<p>​    reflect：pads with reflection of image without repeating the last value on the edge</p>
<p>​    symmeic：pads with reflection of image repeating the last value on the edge</p>
<h4 id="torchvision-transforms-Grayscale-num-output-channels-1"><a href="#torchvision-transforms-Grayscale-num-output-channels-1" class="headerlink" title="torchvision.transforms.Grayscale(num_output_channels=1)"></a><code>torchvision.transforms.Grayscale</code>(<em>num_output_channels=1</em>)</h4><p>将image转为灰度图</p>
<p>参数：<strong>num_output_channels</strong> ，默认为1，也可以为3, 是想要输出图像的channel的个数。</p>
<p>输出：输入的灰度版本。如果nums为1，那么返回的image是单channel，如果是3，返回的image的三个r、g、b三个通道相等。</p>
<p>输出的type：PIL image</p>
<h4 id="torchvision-transforms-Resize-size-interpolation-2"><a href="#torchvision-transforms-Resize-size-interpolation-2" class="headerlink" title="torchvision.transforms.Resize(size, interpolation=2)"></a><code>torchvision.transforms.Resize</code>(<em>size</em>, <em>interpolation=2</em>)</h4><p>将输入的PILimage的大小resize到给定的大小</p>
<p>参数：<strong>size</strong> (<em>sequence</em> <em>or</em> int)期望的输出。如果size是int，那么短的边会匹配到这个数字。ie，如果height&gt;height, 那么image会被缩放为(size*height/width, size). 如果size为sequence，那么大小会被匹配到给定的（h,w）。</p>
<p><strong>interpolation</strong>: 插值的方法，默认为PIL.Image.BILINEAR</p>
<h2 id="Transforms-on-torch-Tensor"><a href="#Transforms-on-torch-Tensor" class="headerlink" title="Transforms on torch.*Tensor"></a>Transforms on torch.*Tensor</h2><h4 id="torchvision-transforms-Normalize-mean-std-inplace-False"><a href="#torchvision-transforms-Normalize-mean-std-inplace-False" class="headerlink" title="torchvision.transforms.Normalize(mean, std, inplace=False)"></a><code>torchvision.transforms.</code>Normalize(<em>mean</em>, <em>std</em>, <em>inplace=False</em>)</h4><p>归一化给定的mean，std来归一化一张tensor image。对于每一个channel进行</p>
<script type="math/tex; mode=display">
\frac{（input[channel - mean[channel]）}{std[channel]}</script><p>参数：mean：每个channel的均值</p>
<p>std: 每个channel的std值</p>
<p>返回：normalized Tensor image</p>
<p>返回类型：Tensor</p>
<p><strong>Note</strong>：不是就地改变输入Tensor</p>
<h2 id="Conversion-Transforms"><a href="#Conversion-Transforms" class="headerlink" title="Conversion Transforms"></a>Conversion Transforms</h2><h4 id="torchvision-transforms-ToPILImage-mode-None"><a href="#torchvision-transforms-ToPILImage-mode-None" class="headerlink" title="torchvision.transforms.ToPILImage(mode=None)"></a><code>torchvision.transforms.ToPILImage</code>(<em>mode=None</em>)</h4><p>将Tensor或者ndarray转换为PILimage</p>
<p>参数：mode:</p>
<p>如果mode没给定：</p>
<p>​    如果输入为4channel，那么默认为RGBA</p>
<p>​    如果输入为3channel，那么默认为RGB</p>
<p>​    如果输入为2channel，那么默认为LA</p>
<p>​    如果输入为1 channel，那么由mode参数确定</p>
<h4 id="torchvision-transforms-ToTensor"><a href="#torchvision-transforms-ToTensor" class="headerlink" title="torchvision.transforms.ToTensor"></a><code>torchvision.transforms.ToTensor</code></h4><p>将PIL image 或者ndarray转换为Tensor</p>
<p>将值范围为【0，255】的PIL image或者ndarray（H/<em>W/</em>C）转换为FloatTensor(C,H,W)并且值范围为【0.0，1.0】，如果the PIL Image属于 one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) 或者 the numpy.ndarray has dtype = np.uint8</p>
<p>其他的，tensors不会进行缩放</p>
<h4 id="FiveCrop和TenCrop"><a href="#FiveCrop和TenCrop" class="headerlink" title="FiveCrop和TenCrop"></a>FiveCrop和TenCrop</h4><p>这两种操作之后,一张图变成五张,一张图变成十张,那么在训练或者测试的时候怎么避免和标签混淆呢<br>思路是,这多个图拥有相同的标签,假如是分类任务,就可以使用交叉熵进行,然后求10张图的平均</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = Compose([</span><br><span class="line">    TenCrop(size), <span class="comment"># this is a list of PIL Images</span></span><br><span class="line">    Lambda(<span class="keyword">lambda</span> crops: torch.stack([ToTensor()(crop) <span class="keyword">for</span> crop <span class="keyword">in</span> crops])) <span class="comment"># returns a 4D tensor</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#In your test loop you can do the following:</span></span><br><span class="line">input, target = batch <span class="comment"># input is a 5d tensor, target is 2d</span></span><br><span class="line">bs, ncrops, c, h, w = input.size()</span><br><span class="line">result = model(input.view(<span class="number">-1</span>, c, h, w)) <span class="comment"># fuse batch size and ncrops</span></span><br><span class="line"></span><br><span class="line">result_avg = result.view(bs, ncrops, <span class="number">-1</span>).mean(<span class="number">1</span>) <span class="comment"># avg over crops</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Zhiwei Liu</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">Kategorien</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiwei Liu</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>



  

  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
