<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <meta property="og:type" content="website">
<meta property="og:title" content="LZW&#39; Blog">
<meta property="og:url" content="liulin1995@github.io/index.html">
<meta property="og:site_name" content="LZW&#39; Blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LZW&#39; Blog">





  
  
  <link rel="canonical" href="liulin1995@github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>LZW' Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LZW' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Startseite</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archiv</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/19/Pytorch训练加速技巧/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/19/Pytorch训练加速技巧/" class="post-title-link" itemprop="url">Pytorch训练加速技巧</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-19 20:27:48 / Geändert am: 20:28:39" itemprop="dateCreated datePublished" datetime="2019-07-19T20:27:48+08:00">2019-07-19</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Pytorch训练模型，如果数据集比较大的话，需要一块GPU。而如何高效利用这块GPU来，来使得训练速度加快是一件非常重要的是。以下是我实践中的经验和搜索到的一些经验：</p>
<ul>
<li><p>dataloader中设置num_worker&gt;1, pin_memory=True。num_worker是用于读取数据用的线程数，pin_memory： 是否将数据放在锁页内存，而不是放在缓存区域，因为GPU的显存都在锁页内存中。</p>
<p>需要注意的是，这个技巧和个人使用的电脑配置有关。在我个人的实验中，通过将num_worker设置为4， pin_memory=True，我训练一个百万图像数据集（224*224）的时间从3个小时缩短到1.5个小时（resnet34为基准）。十几种最好划分出一部分的数据进行“调参”。</p>
</li>
<li><p>使用sgd+momentum这样的需要较少显存的优化器。主要有两个原因，sgd+momentum的效果不比其他的优化器差，反而更能够比Adam达到最优点（即不容易陷在局部极值点和鞍点）。第二个原因是，sgd+momentum只需要对每个参数保存梯度信息和动量信息。而对于类似Adam，额外需要一个二阶信息，这个会加大显存的要求。</p>
</li>
<li><p>使用固态硬盘。我实际中把训练数据放在系统盘，因为系统盘是固态硬盘。把验证集数据放在机械硬盘，毕竟验证过程中可以加大batchsize，且不需要记录梯度信息，显存不会爆炸。</p>
</li>
<li><p>避免将不需要的数据从GPU中拷贝出来。实际中，我们常需要把每个batch的统计信息比如loss，acc或其他数据显示出来。实际上可以在这些数据在GPU中进行累积，在每个epoch结束的 时候显示出来。</p>
</li>
<li><p>torch.cuda.emptyCache()：释放PyTorch的缓存分配器中的缓存内存块，用于防止一不小使用大grad_require=True的变量。同时使用del来删除不需要的变量来释放内存</p>
</li>
<li><p>预先读取下一次迭代用的数据：最近才看到一个，mark。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">作者：巽二</span><br><span class="line">链接：https://zhuanlan.zhihu.com/p/68191407</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br><span class="line"></span><br><span class="line">class data_prefetcher():</span><br><span class="line">    def __init__(self, loader):</span><br><span class="line">        self.loader = iter(loader)</span><br><span class="line">        # self.stream = torch.cuda.Stream()</span><br><span class="line">        self.preload()</span><br><span class="line"></span><br><span class="line">    def preload(self):</span><br><span class="line">        try:</span><br><span class="line">            self.next_data = next(self.loader)</span><br><span class="line">        except StopIteration:</span><br><span class="line">            self.next_input = None</span><br><span class="line">            return</span><br><span class="line">        # with torch.cuda.stream(self.stream):</span><br><span class="line">        #     self.next_data = self.next_data.cuda(non_blocking=True)</span><br><span class="line">            </span><br><span class="line">    def next(self):</span><br><span class="line">        # torch.cuda.current_stream().wait_stream(self.stream)</span><br><span class="line">        data = self.next_data</span><br><span class="line">        self.preload()</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">正常定义train_loader</span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">prefetcher = data_prefetcher(train_loader)</span><br><span class="line">data = prefetcher.next()</span><br><span class="line">i = 0</span><br><span class="line">while data is not None:</span><br><span class="line">    print(i, len(data))</span><br><span class="line">    i += 1</span><br><span class="line">    data = prefetcher.next()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/将python代码打包成exe文件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/将python代码打包成exe文件/" class="post-title-link" itemprop="url">将python代码打包成exe文件</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 17:28:19 / Geändert am: 17:29:29" itemprop="dateCreated datePublished" datetime="2019-07-13T17:28:19+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/编程/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="安装pyinstaller"><a href="#安装pyinstaller" class="headerlink" title="安装pyinstaller"></a>安装pyinstaller</h2><p>pip install pyinstaller</p>
<h2 id="编译成exe"><a href="#编译成exe" class="headerlink" title="编译成exe"></a>编译成exe</h2><p>运行下面的命令行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller **.py</span><br></pre></td></tr></table></figure>
<p>当前目录下的dist文件夹下的项目文件夹会有产生的exe文件。这种方法到处的exe需要很多的附加依赖项，运行的时候也不能脱离那个文件。如果想只生成一个exe，需要加上 -F</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller **.py -F</span><br></pre></td></tr></table></figure>
<p>（note:如果自带界面也可以 -Fw）</p>
<p>经过对比发现单独只生成一个exe文件，启动速度非常慢，并且单独的exe文件也非常大。如何减少体积：</p>
<p><strong>要在虚拟环境里安装pyinstaller</strong>和程序用到的库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#建立虚拟环境</span><br><span class="line">pipenv install</span><br><span class="line">#进入虚拟环境</span><br><span class="line">pipenv shell</span><br><span class="line">#安装模块</span><br><span class="line">pip install 小工具.py里面用到的模块</span><br><span class="line">#打包的模块也要安装</span><br><span class="line">pip install pyinstaller</span><br><span class="line">#开始打包</span><br><span class="line">pyinstaller -Fw E:\test\url_crawler.py</span><br></pre></td></tr></table></figure>
<p>实际效果表明确实能够非常大地减少exe的 </p>
<p>用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<p> 可以加快下载</p>
<p>conda自带了很多的库</p>
<h2 id="拷贝需要的资源文件"><a href="#拷贝需要的资源文件" class="headerlink" title="拷贝需要的资源文件"></a>拷贝需要的资源文件</h2><p>将py文件用的资源，比如图片等放入dist文件夹下的项目文件夹，或和单独exe文件同一个文件夹。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/57674343" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/57674343</a></p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/线性方程组/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/线性方程组/" class="post-title-link" itemprop="url">线性方程组</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 15:42:42 / Geändert am: 15:44:10" itemprop="dateCreated datePublished" datetime="2019-07-13T15:42:42+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>线性方程组问题，通常表达为：</p>
<script type="math/tex; mode=display">
Ax=b</script><p>假设$A \in R^{m*n}, b\in R^m $。通常情况下，如果矩阵$A$可逆，那么该线性方程组的解$x=A^{-1}b$。但是当矩阵A不可解的时候，该线性方程组可能无解或者有无穷多个解。</p>
<h3 id="矩阵的列空间"><a href="#矩阵的列空间" class="headerlink" title="矩阵的列空间"></a>矩阵的列空间</h3><p>如何判断什么时候有唯一解，无解或者无穷多解从矩阵$A$的列空间来看是非常明了的。</p>
<p> 矩阵$A$的列空间为矩阵$A$的列向量构成的向量空间：</p>
<ul>
<li>如果矩阵$A$的列数n为m，且$A$的列向量能够张成$R^m$空间，那么由于$b$也在$R^m$空间，那么肯定有矩阵$A$的列向量线性组成$b$，且该线性组合是唯一的，即此时有唯一解。</li>
<li>如果矩阵$A$的列数n小于m，则$A$的列向量不能够张成$R^m$空间，$A$的列空间会是$R^m$空间的子空间，如果此时$b$恰好在这个子空间中，那么此时线性方程组有解；如果$b$不在该子空间中，就无解。</li>
<li>如果矩阵$A$的列数n大于m，如果$A$的列向量不能够张成$R^m$空间，则结果同上。如果$A$的列空间会是$R^m$空间的子空间，那么此时矩阵$A$的列向量有无穷多的线性组合$b$，此时有无穷多解。</li>
</ul>
<h3 id="伪逆（moore-penrose"><a href="#伪逆（moore-penrose" class="headerlink" title="伪逆（moore-penrose)"></a>伪逆（moore-penrose)</h3><p>如果线性方程组存在无穷多解的时候，可以使用伪逆来求解。伪逆定义为：</p>
<script type="math/tex; mode=display">
A^+ = lim_{\alpha \rightarrow 0}(A^TA + \alpha I)^{-1}A^T</script><p>主要是来自于矩阵的奇异值分解：</p>
<script type="math/tex; mode=display">
A^+ = VD^+U^T</script><p>其中$D^+$是对对角矩阵$D$中非零对角元素取导数得到的。</p>
<p>此时有：</p>
<script type="math/tex; mode=display">
A^+A=VD^+U^TUDV^T \\
= VD^+DV^T \\
= VI^+V^T \\
= I^+</script><p>当矩阵$A$的列数多于行数的时候，使用伪逆求解线性方程会得到一个解$x=A^+y$，该解会是方程所有解中二范数最小的一个。</p>
<p>当矩阵$A$的列数小于行数的时候，使用伪逆的解$x=A^+y$到$y$的距离最短。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/梯度下降法、牛顿法和拟牛顿法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/梯度下降法、牛顿法和拟牛顿法/" class="post-title-link" itemprop="url">梯度下降法、牛顿法和拟牛顿法</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 15:00:24 / Geändert am: 15:01:09" itemprop="dateCreated datePublished" datetime="2019-07-13T15:00:24+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>梯度下降法是机器学习里面最常用的优化方法，简单直接效果还好，因此被广泛应用。今天就来回顾下梯度下降法。</p>
<p>梯度下降法有一个很trivial的想法，比如你在山上，你想要最快的下山，那么你就要走最陡的路下山，即坡度最大的路下山。反映在数学里面，山高为一个目标函数$f(x)$, 你当前的位置是$x_0 $, 当前坡度最大的路对应于当前位置$x$梯度最大的方向，但是由于我们是要下山，我们要反着方向走，即方向为</p>
<script type="math/tex; mode=display">
- \frac{\partial f}{\partial x}</script><p>确定了方向之后，我们需要确定要走的步长$\alpha$, 由于梯度是在$x_0 $处得到的，它只能够表示$f(x)$在点$x_0$出的陡峭程度，如果我们走的步伐太大，脱离的点$x_0$的邻域，那么这个梯度并不会是一个很好的下山方向。因此我们要保证这个步长$\alpha$足够小，此时，我们迈出了下山的第一步：</p>
<script type="math/tex; mode=display">
x_1 = x_0 - \alpha\frac{\partial f}{\partial x}</script><p>一次类推，只要我们一小步一小步的走陡峭的路，很快就能下山：</p>
<script type="math/tex; mode=display">
x_{k+1} = x_k - \alpha\frac{\partial f}{\partial x}</script><p>下面用稍微严谨一点的数学说明下为什么要走负梯度的方向。假设我们在$f(x)$的$x_0 $处走一小步$\Delta x$，</p>
<script type="math/tex; mode=display">
f(x_0 + \Delta x)</script><p>我们的目标是走了这一步后的$f(x_0 + \Delta x)$相比$f(x_0)$下降的最多。首先，我们先在$x_0$处对$f(x)$进行一阶泰勒展开：</p>
<script type="math/tex; mode=display">
f(x)=f(x_0) + f'(x_0)\Delta x + o(f)</script><p>让$f(x)$下降的最多的点即为$x_1$,让要想让近似后的$f(x)$ 最小的话，就要让$f’(x_0)\Delta x$的值最小，考虑到两个向量的内积最小的问题，便要求$\Delta x$ 和梯度是反向的。实际上，只要$\Delta x$和梯度的夹角是大于90度的时候，上面的$argmin \ f(x)$就可以比$f(x_0)$小，当$\Delta x$和梯度的夹角是90度或者$\Delta x$是0的时候，$argmin  \ f(x)$和$f(x_0)$相等。此时令$f(x)$最小的$x$即为$x_{1}$,同时有$x_1 = x_0 + \Delta x$.</p>
<p>更一般的，考虑在$x_k$处对函数$f(x)$进行一阶泰勒展开，则能到的迭代更新公式为：</p>
<script type="math/tex; mode=display">
x_{k+1} = x_k-f'(x_k)</script><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>梯度下降法只是用到了一阶的导数信息，而牛顿法可以用到二阶的导数信息。</p>
<p>首先，我们先在$x_k$处对$f(x)$进行二阶泰勒展开,$\Delta x = x - x_k$：</p>
<script type="math/tex; mode=display">
f(x)=f(x_k) + f'(x_k)\Delta x + \frac{1}{2} f''(x_k)\Delta^2 x+ o(f)</script><p>让$f(x)$下降的最多的点即为$x_{k+1}$，即要$f(x)$取$f’(x)=0$的点，即让：</p>
<script type="math/tex; mode=display">
f'( x)=0</script><script type="math/tex; mode=display">
and \ \ f'( x) = f''(x_k)\Delta x + f'(x_k)</script><p>可得：</p>
<script type="math/tex; mode=display">
\Delta x = -f'(x_k)/f''(x_k) \\
so \\
x_{k+1} = x_k -f'(x_k)/f''(x_k)</script><p>假设$f’(x_k) = g, f’’(x_k)=H$为一个hessian矩阵，那么$\Delta x$的矩阵形式为：</p>
<script type="math/tex; mode=display">
\Delta x =-H^{-1}_k*g_k</script><p>此时$x$的更新公式为：</p>
<script type="math/tex; mode=display">
x_{k+1} = x_k - H^{-1}_{x_k}*g_{x_k}</script><h4 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h4><p>牛顿法的迭代中，需要计算hessian矩阵的逆，这个计算很耗时。而拟牛顿法考虑使用一个矩阵$G$来近似hessian矩阵$H_{x_k}^{-1}$ 。考虑何种的矩阵能够近似hessian矩阵。</p>
<p>按上面的等式(9)。取$x=x_{k+1}$,有：</p>
<script type="math/tex; mode=display">
g_{k+1}=H_k\Delta x + g_k</script><p>即</p>
<script type="math/tex; mode=display">
g_{k+1} - g_k =H_k\Delta x</script><script type="math/tex; mode=display">
 H_k^{-1}(g_{k+1}-g_k)=\Delta x</script><p>这个是hessian矩阵满足的条件，被称为拟牛顿条件。</p>
<p>如果$H_k$矩阵是正定的，那么可以保证牛顿法搜索是下降的，这个是因为根据（11）有</p>
<script type="math/tex; mode=display">
f(x) = f(x_k) - g_k^TH^{-1}_kg_k + \frac{1}{2}g_k^TH^{-1}_k HH^{-1}_kg_k \\
f(x) = f(x_k) - \frac{1}{2}g_k^TH^{-1}_kg_k</script><p>因为$H_k$矩阵是正定的，$H^{-1}_k$也会是正定的，那么有$g_k^TH^{-1}_kg_k$大于0，总有$f(x)&lt;f(x_k)$，即牛顿法是下降的。</p>
<p>拟牛顿法是牛顿法的近似，选择$G_{k}$作为$H^{-1}_k$的近似，要求矩阵$G_k$满足同样的条件。首先$G_k$是正定的，其次，$G_k$满足上面的拟牛顿条件，即</p>
<script type="math/tex; mode=display">
G_k(g_{k+1} - g_k )=\Delta x</script><p>这样子的方法被统称为拟牛顿法。同时可以使用下面的方式来更新$G_{k+1}$:</p>
<script type="math/tex; mode=display">
G_{k+1}= G_k + \Delta G</script><p>下面介绍BFP算法</p>
<p>假设每一步中的$G_{k+1}$是由$G_{k}$加上两个附加项构成的：</p>
<script type="math/tex; mode=display">
G_{k+1}=G_k + P_k + Q_k</script><p>要求：</p>
<script type="math/tex; mode=display">
G_{k+1}(g_{k+1} - g_k )=\Delta x \\
(G_k + P_k + Q_k)(g_{k+1} - g_k )=\Delta x</script><p>可以让$P_k (g_{k+1} - g_k )=\Delta x$且$Q_k(g_{k+1} - g_k )=-G_k(g_{k+1} - g_k )$ 使得更新后$G_{k+1}$满足拟牛顿条件。</p>
<p>事实上，这样子的满足条件的$P_k$和$Q_k$并不难找。例如取：</p>
<script type="math/tex; mode=display">
P_k=\frac{\Delta x^T\Delta x }{\Delta x ^T( g_{k+1} - g_k )}</script><script type="math/tex; mode=display">
Q_k = -\frac{G_k( g_{k+1} - g_k )( g_{k+1} - g_k )^TG_k}{( g_{k+1} - g_k )^TG_k( g_{k+1} - g_k )}</script><p>那么可以得到</p>
<script type="math/tex; mode=display">
G_{k+1}= G_k +　\frac{\Delta x^T\Delta x }{\Delta x ^T( g_{k+1} - g_k )}　-\frac{G_k( g_{k+1} - g_k )( g_{k+1} - g_k )^TG_k}{( g_{k+1} - g_k )^TG_k( g_{k+1} - g_k )}</script><p>BFGS算法是一个很常见的算法，用的是用一个正定矩阵$B_k$ 来近似$H_k$ ,同样需要满足上面说的拟牛顿条件。类似BFP算法，也有</p>
<script type="math/tex; mode=display">
B_{k+1}=B_k + P_k + Q_k</script><p>考虑使$P_k,Q_k$满足：</p>
<script type="math/tex; mode=display">
P_k\Delta x = (y_{k+1} - y_{k}) \\
Q_k \Delta x = -B_k\Delta x</script><p>找到满足条件的$P_k,Q_k$，得到BFGS的算法矩阵$B_{k+1}$ 的迭代公式：</p>
<script type="math/tex; mode=display">
B_{K+1}=B_k + \frac{( g_{k+1} - g_k )( g_{k+1} - g_k )^T}{( g_{k+1} - g_k )^T\Delta x} - \frac{B_k\Delta x \Delta x^T B_k}{\Delta^TB_k\Delta x}</script><p>实际的BFGS的算法中，有一个确定步长的步骤，即首先确定更新的方向$-B_k^{-1} g_k$, 然后求一个步长$\alpha $,求法如下：</p>
<script type="math/tex; mode=display">
f(x_k + \alpha_k (-B_k^{-1} g_k)) = min f(x_k + \alpha (-B_k^{-1} g_k)) \\
x_{k+1} = x_k+\alpha_k(-B_k^{-1} g_k)</script>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/operator/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/operator/" class="post-title-link" itemprop="url">soft-thresholding operator</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:58:42 / Geändert am: 14:59:17" itemprop="dateCreated datePublished" datetime="2019-07-13T14:58:42+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>让$f(x) = \lambda |x|_1$ ， 那么$f$的近似投影被定义为：</p>
<script type="math/tex; mode=display">
prox_f(x) = argmin_x\{\frac{1}{2}|x-z|_2^2 + \lambda|z|_1\}</script><p>该问题的最优性条件为：</p>
<script type="math/tex; mode=display">
0 \ \in \Delta(\frac{1}{2}|x-z|_2^2)+\partial (\lambda|z|_1)</script><p>由于$l1 \ norm$是可以分离的，因此我们可以单独考虑x中的每一个成分。</p>
<p>首先考虑$z_i \neq 0$的情况，此时有$\partial (\lambda|z|_1)=sign(z_i)$并且最优的$z_i^*$为：</p>
<script type="math/tex; mode=display">
0 = z_i - x_i + \lambda sign(z_i) \leftrightarrow 0 \in z - x + \lambda|x|_1</script><p>需要指出是，如果$z_i^<em> &lt;0$那么$x_i &lt;-\lambda$。同时如果$z_i^</em> &gt;0$那么$x_i &gt;\lambda$。因此，$|x_i| &gt;\lambda$ 和$sign(z_i^*) = sign(x_i)$.代入道先前的等式得到：</p>
<script type="math/tex; mode=display">
z_i^* = x_i - \lambda sign(x_i)</script><p>然后考虑$z_i = 0$的情况。此时$l_1 \ norm$的次微分为区间$[-1,1]$，并且最优性条件为：</p>
<script type="math/tex; mode=display">
0 = - x_i + \lambda [-1,1] \leftrightarrow x_i \in [-\lambda,\lambda] \leftrightarrow |x|_1 \le \lambda</script><p>将两个情况放在一起得到了：</p>
<script type="math/tex; mode=display">
[porx_f(x)]_i = z_i^* = \begin{cases}
    0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if |x_i| \le \lambda\\
    x_i - \lambda sign(x_i) \ \ \  if |x_i| >\lambda\\
\end{cases}</script><p>等式(6)同样可以写为：</p>
<script type="math/tex; mode=display">
[porx_f(x)]_i = sign(x_i)max(|x_i|-\lambda,0) \\
\ \ \ \ \ \ \ \ \ \ = sign(x_i)(|x_i| - \lambda)_+</script>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/Thresholding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/Thresholding/" class="post-title-link" itemprop="url">Singular Value Thresholding</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:57:40 / Geändert am: 14:59:44" itemprop="dateCreated datePublished" datetime="2019-07-13T14:57:40+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>SVT（奇异值阈值）奇异值收缩（singular value shrinkage)</p>
<p>考虑一个秩为r的矩阵$X\in R^{n1*n2}$的奇异值分解如下：</p>
<script type="math/tex; mode=display">
X= U \Sigma V^*，\Sigma = diag(\{\sigma_i\}_{1 \le i \le r})</script><p>其中$U,V$分别是$n1<em>r，n2</em>r$的正交矩阵，奇异值$\sigma_i$非负。</p>
<p>对于每一个$\tau \ge 0$，有软阈值操作$D_\tau$：</p>
<script type="math/tex; mode=display">
D_\tau(X) ：= UD_\tau(\Sigma)V^*, \ \ \ D_\tau(\Sigma)= diag(\{(\sigma_i - \tau)_+\}_{1 \le i \le r})</script><p>可以看出，这个软阈值操作是作用在奇异值上的，使他们趋于0，这也是这个被叫做奇异值收缩的原因。</p>
<p>对于奇异值收缩$D_\tau$ 有一个重要的结论：</p>
<script type="math/tex; mode=display">
D_\tau（Y)= arg\ min_X \frac{1}{2} |X-Y|_F^2 + \tau|X|_*</script>
          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/Softmax求导/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/Softmax求导/" class="post-title-link" itemprop="url">Softmax求导</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:56:31 / Geändert am: 14:57:08" itemprop="dateCreated datePublished" datetime="2019-07-13T14:56:31+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>softmax 可以看作是logistic “one versus all”的多分类版本，和普通的“one versus all”不同的是，softmax是通过选择最大的输出概率值来做最后的决策任务，因此不存在”混淆区域“</p>
<p>softmax的公式为：</p>
<script type="math/tex; mode=display">
p(Y=k|x)=\frac{exp(w_k * x)}{1+\Sigma_i^{K-1}{exp(w_i * x)}}  \\
p(Y=K|x)=\frac{1}{1+\Sigma_i^{K-1}{exp(w_i * x)}}</script><p>为什么$p(Y=K|x)=\frac{1}{1+\Sigma_i^{K-1}{exp(w_i <em> x)}}$？是因为，softmax是存在参数冗余的，即一开始所有的类别k都有一个$w_k$的，但是，如果对上面下面都除以一个$exp(w_K</em>x)$的话，就得到上面的公式：</p>
<script type="math/tex; mode=display">
p(Y=k|x)=\frac{exp(w_k * x)}{\Sigma_i^{K}{exp(w_i * x)}}  \\
上下都除以exp(w_K*x)得到： \\
p(Y=k|x)_{k\ne K}= \frac{exp((w_k-w_K)*x)}{\Sigma_i^{K}{exp((w_i-w_K) * x)}}=\frac{exp((w_k-w_K)*x)}{1+\Sigma_i^{K-1}{exp((w_i-w_K) * x)}}\\
p(Y=k|x)_{k = K}= \frac{exp((w_K-w_K)*x)}{\Sigma_i^{K}{(w_i-w_K) * x}}=\frac{1}{1+\Sigma_i^{K-1}{exp((w_i-w_K) * x)}}</script><p>令新的到的$w_i- w_K$为新的$w_i$即得到公式(1)。</p>
<p>得到softmax的参数的过程是通过<strong>最大似然函数</strong>得到的。</p>
<script type="math/tex; mode=display">
likelihood = \Pi_i^N\Pi_k^K1\{y_i=k\}p(Y=k|x)</script><p>其中$1\{y_i=k\}在$$\{\}$内表达式为真的时候取1。由于有多个连乘，做法是对其log，得到它的对数似然函数：</p>
<script type="math/tex; mode=display">
loglikelihood = \Sigma_i^N\Sigma_k^K \ log \ 1\{y_i=k\} \ p(Y=k|x)\\
=\Sigma_i^N\Sigma_k^K log \ \frac{1\{y_i=k\}*exp(w_k * x_i)}{1+\Sigma_j^{K-1}{exp(w_j * x_i)}}\\
=\Sigma_i^N\Sigma_k^K\{\{1\{y_i=k\}*w_k*x_i -log\{1+\Sigma_j^{K-1}{exp(w_j * x)}\}\}\}</script><p>此外，$log\{1+\Sigma_i^{K-1}{exp(w_i * x)}\}<script type="math/tex">对于</script>w_k$求导有：</p>
<script type="math/tex; mode=display">
\frac{\partial log\{1+\Sigma_j^{K-1}{exp(w_i * x_i)}}{\partial w_k}=\frac{1}{1+\Sigma_j^{K-1}{exp(w_j * x_i)}}*\frac{\partial (1+\Sigma_j^{K-1}{exp(w_j * x_i)})}{\partial w_k} \\
= \frac{1}{1+\Sigma_j^{K-1}{exp(w_j * x_i)}}*exp(w_k*x_i)*x=p(Y=k|x_i)*x_i</script><p>因此对对数似然函数求导为：</p>
<script type="math/tex; mode=display">
\frac{\partial loglikellihood}{\partial w_j}=\Sigma_i^N\Sigma_k^K\{1\{y_i=k\}w_k*x_i -log\{1+\Sigma_j^{K-1}{exp(w_j * x)}\}\} \\
= \Sigma_i^N \{1\{y_i=j\}x_i -x_i*p(Y=j|x_i)\}\}\\
=\Sigma_i^N x_i*(1\{y_i=j\} - p(Y=j|x_i))\}</script><p>可知，样本是否属于j类，样本i对于$w_j$的梯度贡献不同(这里假设是最小化log likelihood,需要对梯度取反）：</p>
<script type="math/tex; mode=display">
if \ x_i \in class_j \\
the \ contribution \ of \ x_i \ to \ \partial_{w_j} \ is: \ -x_i(1-p(y=j|x_i)) \\
else:\\
the \ contribution \ of \ x_i \ to \ \partial_{w_j} \ is: \ x_i*p(Y=j|x_i)</script><p>回顾到感知机的更新为用错误的样本对权重$w$进行更新，</p>
<script type="math/tex; mode=display">
W \leftarrow W - (-y_ix_i) \\
b \leftarrow W - (-y_i)</script><p>可以看到softmax除了对错误样本进行更新和感知机是吻合的，不同指出是softmax也用正确的样本来更新权重。</p>
<p>可以把$x_i(1-p(y=j|x_i))$ 重新写为：$-x_i(p(y=j|x_i) - 1)$</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/GBDT提升树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/GBDT提升树/" class="post-title-link" itemprop="url">GBDT提升树</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:54:36 / Geändert am: 14:55:13" itemprop="dateCreated datePublished" datetime="2019-07-13T14:54:36+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提升树是以分类树或者回归树为基本分类器的提升方法。提升方法采用的加法模型和前向分布算法。提升树模型可以表示为决策树的加法模型：</p>
<script type="math/tex; mode=display">
f_M(x) = \Sigma_{m=1}^{M} T(x;W_m)</script><p>梯度提升树采用的是前向分布算法。第m步骤的模型为：</p>
<script type="math/tex; mode=display">
f_m(x;W_m)=f_{m-1} + T(x;W_m)</script><p>则，通过经验风险极小化来确定下一棵数的参数$W_m$, </p>
<script type="math/tex; mode=display">
\hat{W}_m  = argmin \ \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)+T(x_n;W_m))</script><h2 id="一阶的GBDT"><a href="#一阶的GBDT" class="headerlink" title="一阶的GBDT"></a>一阶的GBDT</h2><p>考虑一阶泰勒展开，对$obj(m)= \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)+T(x_n;W_m)$在$f_{m-1}(x)$处展开有：</p>
<script type="math/tex; mode=display">
obj(m) = \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)+T(x_n;W_m) \thickapprox  \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)) + \frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }*T(x_n;W_m) + \varTheta (f(x_n))</script><p>对近似一阶展开最小化，可知：$L(y_n,f_{m-1}(x_n))$,$\frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }$都是constant，因为为了最小化(4)，只要求：</p>
<script type="math/tex; mode=display">
T(x_n;W_m) = -1*\frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }</script><p>这个就得到了梯度提升树的算法，新的一颗树是通过拟合负梯度得到的。</p>
<p>需要注意的是，当$L(y,f_m(x))$选为平方函数的时候，上述的负梯度即残差。</p>
<h2 id="二阶的GBDT"><a href="#二阶的GBDT" class="headerlink" title="二阶的GBDT"></a>二阶的GBDT</h2><p>考虑二阶展开，有：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N L(y_n,f_{m-1}(x_n)) + \frac{\partial L(y_n,f(x_n))}{\partial f _{m-1}(x_n) }*T(x_n;W_m)+ \frac{1}{2} \frac{\partial L(y_n,f(x_n))}{\partial^2f _{m-1}(x_n) }*T^2(x_n;W_m) +\varTheta (f(x_n))</script><p>可以缩写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N   L(y_n,f_{m-1}(x_n)) + g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)</script><p>由于函数中的常量在优化过程中不起影响，因此可以省略，(7)可以进一步写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N    g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)</script><p>在更一般的机器学习的目标函数中，会通过添加一个正则项，来达到一个最小化结构化误差的作用，因此(8)进一步写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N    g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)+\Omega(T(x;W_m))</script><p>最常见的一个二阶的GBDT模型为Xgboost模型。</p>
<h2 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h2><p>这里假设一棵决策树，叶子节点个数为$M$,该决策树是由叶子节点上的值组成的向量$\omega \in R^M$，以及一个将一个特征向量映射到叶子节点的索引函数$q:R^d \rightarrow \{1,2,…,M\}$ 组成的。因此一个决策树可以重新写为：$T(x)=\omega_{q(x)}$</p>
<p>此时正则项定义为：$\Omega(T) = \gamma M + \frac{1}{2}<em> \lambda</em> \Sigma_{j=1}^M\omega^2$来定义。此时为叶子j定义一个样本集合为：$I_j = \{n|q(x_n)=j\}$</p>
<p>上面的目标函数可以重新写为：</p>
<script type="math/tex; mode=display">
obj(m) \thickapprox   \Sigma_{n=1}^N    g_n*T(x_n;W_m) + \frac{1}{2}*h_n*T^2(x_n;W_m)+\Omega(T(x;W_m)) \\
obj(m)\thickapprox   \Sigma_{n=1}^N    g_n*\omega_{q(x_n)} + \frac{1}{2}*h_n*\omega^2_{q(x_n)}+\gamma M + \frac{1}{2}* \lambda* \Sigma_{j=1}^M\omega^2 \\
obj(m)\thickapprox   \Sigma_{j=1}^M  (\Sigma_{n \in I_j}g_n)  *\omega_j + \frac{1}{2}*(\Sigma_{n \in I_j}h_n+\lambda)*\omega^2_{j}+\gamma M</script><p>定义$G_n = \Sigma_{n \in I_j}g_n,H_n=(\Sigma_{n \in I_j}h_n)$,那么上面的目标函数可以写为：</p>
<script type="math/tex; mode=display">
obj(m)\thickapprox   \Sigma_{j=1}^M  G_n *\omega_j + \frac{1}{2}*(H_n+\lambda)*\omega^2_{j}+\gamma M</script><p>对于一个单变量二次方程式:</p>
<script type="math/tex; mode=display">
argmin_x Gx+\frac{1}{2}Hx^2 = - \frac{G}{H}, H>0\\
min_x Gx+\frac{1}{2}Hx^2 = - \frac{G^2}{H}</script><p> 假设树的结构是固定的，那么每个叶子的最佳权重为：</p>
<script type="math/tex; mode=display">
\omega_j^* = -\frac{G_j}{H_j+\lambda}</script><script type="math/tex; mode=display">
obj = -\frac{1}{2}\Sigma_{j=1}^M \frac{G_j^2}{H_j + \lambda} + \gamma M</script><p>如果给定一个树的结果，可以用上面的公式计算出得分，并且为每个叶子赋值。但是问题是，如何找到一个合适的结构。</p>
<p>xgb中的方法是贪婪的</p>
<ul>
<li><p>树从深度0开始</p>
</li>
<li><p>对树的每一个叶子节点，尝试增加一个划分。目标函数的变化程度为：</p>
<script type="math/tex; mode=display">
Gain= \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma</script></li>
</ul>
<p>对于每一个节点，遍历所有的特征：</p>
<ul>
<li>对于每一个特征，根据特征值对样本进行排序</li>
<li>使用线性扫描的方法去决定特征的最佳划分</li>
<li>选择所有特征中最佳的特征</li>
</ul>
<p>回到GBDT，生成的树要加到原来的模型上：</p>
<script type="math/tex; mode=display">
f_m(x;W_m)=f_{m-1} + \epsilon T(x;W_m)</script><p>这里$\epsilon$被称为步长或者收缩，一般设置为0.1左右。这个意味着在每一步没有做最好的优化，保留了后面的轮数，有助于防止过拟合。</p>
<p>除了步长外，xgb还有其他的防止过拟合的措施：</p>
<ul>
<li><p>Early Stopping：本质是在某项指标达标后就停止训练，也就是设定了训练的轮数</p>
</li>
<li><p>Subsampling：无放回抽样，具体含义是每轮训练随机使用部分训练样本，其实这里是借鉴了随机森林的思想</p>
</li>
<li><p>colsample_bytree: 训练的过程中，使用的特征以一定的比例从所有的特征中采样。</p>
</li>
<li><p>max_depth: 树的深度，树越深越容易过拟合</p>
</li>
<li><p>min_child_weight: 值越大，越容易欠拟合。</p>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>  [李航《统计学习方法》]: </p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/拉格朗日对偶性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/拉格朗日对偶性/" class="post-title-link" itemprop="url">拉格朗日对偶性</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:53:25 / Geändert am: 14:53:55" itemprop="dateCreated datePublished" datetime="2019-07-13T14:53:25+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>拉格朗日对偶是约束最优化问题中最常见的一种方法，将带约束的问题转化为不带约束的问题，并进行求解。除此之外，拉格朗日对偶性有这非常好的性质，例如：</p>
<ul>
<li>对偶问题可以给出原问题的一个下界</li>
<li>无论原问题是否是凸的，对偶问题都是凸的</li>
<li>当满足一定的条件的时候，原始问题和对偶问题完全等价。</li>
</ul>
<h3 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h3><p>首先我们先提到原始问题。考虑这么一个带约束的问题：</p>
<script type="math/tex; mode=display">
min_{x} f(x)\\
s.t \ \ c_i(x) \le 0,\ i=1,2,...,k \\
\ \ \ \  h_j(x) = 0, \ j=1,2,...,m</script><p>同时引入广义拉格朗日函数 ：</p>
<script type="math/tex; mode=display">
L(x,\alpha,\beta) = f(x)+ \sum_{i=1}^k{\alpha_i c_i(x)} + \sum_{j=1}^m{\beta_j h_j(x)}</script><p>其中$\alpha_i$,$\beta_i$是拉格朗日乘子，$\alpha_i\ge0$。那么考虑以下的函数：</p>
<script type="math/tex; mode=display">
\theta_p(x) = max_{\alpha,\beta;\alpha\ge0}L(x,\alpha,\beta)</script><p>假设存在一个$x$,$x$会是以下四种情况之一：</p>
<ol>
<li>$x$满足约束$c_i(x)\le0,h_j(x)=0$</li>
<li>$x$不满足约束$c_i(x) \le 0 $</li>
<li>$x$不满足约束$h_j(x)=0$</li>
<li>$x$不满足约束$h_j(x)=0$和$h_j(x)=0$</li>
</ol>
<p>考虑第一种情况，此时$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\alpha_i = 0$,且$\beta_i$值任意，此时$\theta_p(x)$等价于$f(x)$.即此时$\theta_p(x)$等价于原问题。</p>
<p>考虑第二种情况，此时$c_i(x) &gt; 0$.$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\alpha_i \rightarrow +\infty$,且$\beta_i$值任意，此时$\theta_p(x) \rightarrow +\infty$。</p>
<p>考虑第三种情况，此时$h_j(x) \ne 0$,$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\beta_i \rightarrow +\infty或-\infty$,且$\alpha_i$值为0.此时，$\theta_p(x) \rightarrow +\infty$。</p>
<p>考虑第四种情况，此时$h_j(x) \ne 0$，$c_i(x) &gt; 0$,$\theta_p(x)$里面的max为了最大化$L(x,\alpha,\beta)$, 会让$\beta_i \rightarrow +\infty或-\infty$,且$\alpha_i \rightarrow +\infty$，$\theta_p(x) \rightarrow +\infty$。</p>
<p>因此可以看出$\theta_p(x)$的性质，</p>
<script type="math/tex; mode=display">
\theta _p=\begin{cases}
    f\left( x \right)\\
    +\infty\\
\end{cases}</script><p>如果考虑问题</p>
<script type="math/tex; mode=display">
min_x  \theta_p(x)</script><p>该问题和原问题是等价的，即它们有等价的解或者同样无解。这样子就得到了广义拉格朗日函数的极小极大问题。定义原问题的最优解为：</p>
<script type="math/tex; mode=display">
p^* = min_x  \theta_p(x)</script><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>定义问题：</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta)=min_x L(x,\alpha,\beta)</script><p>再考虑极大化$\theta_D(\alpha,\beta)=min_x=L(x,\alpha,\beta)$,即</p>
<script type="math/tex; mode=display">
max_{\alpha_i,\beta_j;\alpha_i\ge 0} \theta_D(\alpha,\beta) = max_{\alpha_i,\beta_j;\alpha_i\ge 0} min_x L(x,\alpha,\beta)</script><p>该问题被称为广义拉格朗日函数的极大极小问题。该问题表达为带约束的优化问题</p>
<script type="math/tex; mode=display">
max_{\alpha_i,\beta_j} min_x L(x,\alpha,\beta) \\
s.t \ \ \alpha_i\ge 0, \ \ i=1,2,...,k</script><p>该带约束的问题被称为原始问题的对偶问题。同时定义该对偶问题的最优解为：</p>
<script type="math/tex; mode=display">
d^* = max_{\alpha_i,\beta_j} \theta_D(\alpha,\beta)</script><h3 id="对偶问题和原始问题的关系"><a href="#对偶问题和原始问题的关系" class="headerlink" title="对偶问题和原始问题的关系"></a>对偶问题和原始问题的关系</h3><h4 id="弱对偶性和强对偶性"><a href="#弱对偶性和强对偶性" class="headerlink" title="弱对偶性和强对偶性"></a>弱对偶性和强对偶性</h4><p>对偶问题和原始问题的最优解满足以下关系：</p>
<script type="math/tex; mode=display">
d^* \le p^*</script><p>证明很简单，即：</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta)=min_x L(x,\alpha,\beta) \le L(x,\alpha,\beta) \le max_{\alpha,\beta;\alpha\ge0}L(x,\alpha,\beta)=\theta_p(x)</script><p>即</p>
<script type="math/tex; mode=display">
\theta_D(\alpha,\beta) \le \theta_p(x)</script><p>则有</p>
<script type="math/tex; mode=display">
max_{\alpha,\beta} \  \theta_D(\alpha,\beta) \le min_x \ \theta_p(x) \\
d^* \le p^*</script><p>这个性质也被称为<strong>弱对偶性</strong>，对所有的优化问题成立，即使原始问题非凸。相对于弱对偶性，也有<strong>强对偶性</strong>：</p>
<script type="math/tex; mode=display">
d^* = p^*</script><h4 id="Slater条件"><a href="#Slater条件" class="headerlink" title="Slater条件"></a>Slater条件</h4><p>考虑原始问题和对偶问题。假设函数$f(x) , c_i(x)$是凸函数，并且$h_j(x)$是仿射函数，并且不等式$c_i(x)$是严格可行的,即存在x,对于所有的i有$c_i(x) &lt; 0$则存在$x^<em>,\alpha^</em>,\beta^<em>$,使得$x^</em>$是原问题的最优解，$\alpha^<em>,\beta^</em>$是对偶问题的最优解，同时有：</p>
<script type="math/tex; mode=display">
d^* = p^* = L(x^*,\alpha^*,\beta^*)</script><p><strong>Note</strong>, 该slater条件对应SVM即要求数据集是线性可分的；如果数据是线性不可分的，那么此时使用SVM寻找分隔超平面也失去了意义。</p>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>对于原始问题和对偶问题，假设假设函数$f(x) , c_i(x)$是凸函数，并且$h_j(x)$是仿射函数，并且不等式$c_i(x)$是严格可行的，则$x^<em>$是原问题的最优解，$\alpha^</em>,\beta^<em>$是对偶问题的最优解的充分必要条件是$x^</em>$$\alpha^<em>,\beta^</em>$满足以下的KKT条件：</p>
<script type="math/tex; mode=display">
\begin{cases} \Delta_x L(x^*,\alpha^*,\beta^*) =0
    \\ \Delta_\alpha L(x^*,\alpha^*,\beta^*) =0
    \\ \Delta_\beta L(x^*,\alpha^*,\beta^*) =0
     \\ \alpha_i^*c_i(x)=0, i=1,2,...,k 
     \\ c_i(x^*) \le 0 , i=1,2,...,k 
     \\ \alpha_i \ge 0 , i=1,2,...,k 
     \\ h_j(x^*)=0, j=1,2,...,m
\end{cases}</script><p>其中，$\alpha_i^<em>c_i(x)=0, i=1,2,…,k $被称为是KKT条件的对偶互补条件。由此条件可知：若$\alpha_i^</em> &gt;0$,则$c_i(x^*)=0$.</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>当原始问题为凸优化问题的时候，其对偶问题的强对偶性与KKT条件是互为充要的。</p>
<p>当原始问题不为凸优化问题是，利用其对偶问题也可以得到原问题最优解的下界。</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://blog.csdn.net/qq_32742009/article/details/81413068" target="_blank" rel="noopener">https://blog.csdn.net/qq_32742009/article/details/81413068</a></p>
<p>统计学习方法附录C</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="liulin1995@github.io/2019/07/13/基于梯度的优化算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LZW' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/07/13/基于梯度的优化算法/" class="post-title-link" itemprop="url">基于梯度的优化算法</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-07-13 14:51:43 / Geändert am: 15:03:26" itemprop="dateCreated datePublished" datetime="2019-07-13T14:51:43+08:00">2019-07-13</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h4><p>使用整个训练集来优化权重，意思是用训练集中的所有样本的loss来更新模型的权重。此时，权重的梯度为：</p>
<script type="math/tex; mode=display">
\partial W = \frac{1}{N} \partial\Sigma_i^N \ L(f(x^{i}),y^{i}) \\
 =  \frac{1}{N} \Sigma_i^N \partial\ L(f(x^{i}),y^{i})</script><p>可以看出权重的梯度是每个每个样本的梯度的期望。</p>
<p>此时的更新过程为：</p>
<script type="math/tex; mode=display">
W := W -\eta \frac{1}{N} \Sigma_i^N \partial \ L(f(x^{i}),y^{i})</script><p>如果样本的个数的是10000， 100000个的时候，需要先遍历所有的样本在对权重$W$更新，这个是非常耗时的。因此有人提出了SGD</p>
<h4 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h4><p>相比与batch gradient descent方法， SGD不适用全部的样本来估计权重的梯度，而是使用的一小部分的样本。主要的原因是一小部分的样本的梯度均值可以近似真实的梯度，而这部分样本的数量越多，估计的梯度越近似真实的梯度。另一方面，用小部分样本来估计的梯度是带有噪声的，在一定程度也起到了正则化的作用。</p>
<p>此时的梯度为：</p>
<script type="math/tex; mode=display">
\partial W = \frac{1}{N} \partial\Sigma_i^m \ L(f(x^{i}),y^{i})</script><p>此时的更新过程为：</p>
<script type="math/tex; mode=display">
W := W -\eta \frac{1}{N} \Sigma_i^m \partial \ L(f(x^{i}),y^{i})</script><p>实际过程中，每次选取一部分的样本来估计梯度，并进行更新，这部分被叫做一个迭代；如果一整个的训练集都使用一次，就叫经过了一个 epoch。注意，每个epoch，样本都要被打散。</p>
<h4 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD+Momentum"></a>SGD+Momentum</h4><p>SGD方法存在这一个“震荡”的问题，如下图：</p>
<p><img src="E:\刘志伟的文件\论文笔记\机器学习\assets\1562846876860.png" alt="1562846876860"></p>
<p>可以看到在更新的过程中，梯度下降的迭方向会偏离实际中真正的下降方向。momentum就是为了克服</p>
<p>这个震荡的问题。</p>
<script type="math/tex; mode=display">
v_{t} = \eta v_{t-1} + (1-\eta)\partial W_t \\
W_t \ := W_{t-1}-\eta v_{t}</script><p>动量可以看作是迭代过程中下降方向，震荡的方向被消除了，使得权重更新的方向变得一致。</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>梯度下降中存在一个问题，如果使用相同的学习率，对于权重W中每一个元素$W_i$,如果他们的梯度值有较大的差别的时候，会导致权重在梯度值较小的维度上迭代过慢。Adagrad方法根据权重在每个维度上梯度值的大小来调整各个维度上的学习率，来避免统一学习率难以适应所有维度的问题。</p>
<script type="math/tex; mode=display">
s_t = s_{t-1} + \partial W_t \odot \partial W_t \\
W_t \ := W_{t-1}-\frac{\eta}{\sqrt{s_t}}\partial W_t</script><p>Adagrad存在一个问题，有与$s_t$一致累加着梯度按元素平方的和，因此如果权重的某个元素的偏导数一直较大，会使得学习率下降较快；反之，如果某个元素的偏导数一直较小，学习率则会下降较慢。所以Adagrad在后期的时候，比较难找到一个合适的解。</p>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>由于Adagrad存在后期学习率低的问题，RMSprop对其做了一点小小的改动：</p>
<script type="math/tex; mode=display">
s_t =  \gamma s_{t-1} + (1-\gamma) \ \partial W_t \odot \partial W_t \\
W_t \ := W_{t-1}-\frac{\eta}{\sqrt{s_t}}\partial W_t</script><p>RMSprop对这些梯度的平方做指数加权移动平均，使得每个权重的学习率不会一直降低或者不变。</p>
<p>可以消除梯度下降过程中的摆动，包括mini-batch，允许使用更大的$\alpha$,加快算法学习速度</p>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>Adadelta也针对Adagrad做了改进，同样的，Adadelta也是了梯度平方的加权移动平均。但是它与RMSprop不同的是Adadelta还维护了一个额外的状态变量$\Delta W_t$ 来计算自变量的变化值：</p>
<script type="math/tex; mode=display">
W_t^/ = \sqrt{\frac{\Delta W_{t-1} + \epsilon}{s_t + \epsilon}} \odot W_t</script><p>然后更新权重：</p>
<script type="math/tex; mode=display">
W_t = W_{t-1} -W_t^/</script><p>最后使用$\Delta W_t$来记录权重变化量的指数加权移动平均：</p>
<script type="math/tex; mode=display">
\Delta W_t = \rho\Delta W_{t-1} + (1-\rho)W_t^/  \odot W_t^/</script><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adam算法是将momentum和RMSprop结合起来</p>
<script type="math/tex; mode=display">
v_{t} = \beta_1 v_{t-1} + (1-\beta_1)\partial W_t \\
s_{t} = \beta_2 s_{t-1} +(1-\beta_2)\partial W_t \\
\hat{v}_{t} = \frac{v_{t}}{1-\beta_1^t}\\
\hat{s}_{t} = \frac{S_{t}}{1-\beta_2^t} \\
W_t:= W_{t-1}- \frac{\alpha \hat{v}_{t}}{\sqrt{\hat{s}_{t}+\epsilon}}</script><p>$\beta_1$常用为0.9，$\beta_2$常用为0.999，而$\epsilon$为10^-8。Adam还对动量项和 梯度平方项做了偏差修正，然后使用修正后的变量对权重进行更新。</p>

          
        
      
    </div>

    

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Zhiwei Liu">
            
              <p class="site-author-name" itemprop="name">Zhiwei Liu</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">27</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">Kategorien</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">29</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/liulin1995" title="GitHub &rarr; https://github.com/liulin1995" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          

          
        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiwei Liu</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>



  

  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
