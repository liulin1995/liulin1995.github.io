<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>deep learning | LZW&#39; Blog</title>
  <meta name="author" content="John Doe">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="LZW&#39; Blog">

  
    <meta property="og:image" content>
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
    <script src="/js/marked.js"></script>
    <script src="/js/comment.js"></script>
    <script src="/js/timeago.min.js"></script>
    <script src="/js/highlight.min.js"></script>
	<script src="/js/spin.min.js"></script>
  
  <!-- analytics -->
  



</head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">LZW&#39; Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      

<!-- title -->
<div class="page-header page-header-inverse ">
  <h1 class="archive-title-category title title-inverse ">deep learning</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      
         <!-- display as entry -->
	     <div class="mypage">
	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Cyclical-Learning-Rates-for-Training-Neural-Networks/" >Cyclical Learning Rates for Training Neural Networks</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h2 id="paper-details"><a href="#paper-details" class="headerlink" title="paper details"></a>paper details</h2><p>深度学习中有一个常识是，学习率在训练的过程中需要逐渐减小。但是这篇文章却给出了一个让人惊讶的事实，就是训练过程中的学习率如果是多变（rise and fall）的是有益于训练的。因此作者建议学习率在一个范围内周期变化，而不是将其设置为固定值。</p>
<p>Cyclical Learning Rates来源于这么一个观察：学习率的增加虽然会带来短期的副作用但是长期来看是有益的。因此这种观察引出了让学习率在一定范围内变化而不是采用逐步固定或指数递减值的想法。即设置一个最大和最小的边界，然后学习率在里面循环变化。如下图的 triangular learning rate policy：</p>
<p><img src="/images/assets/1557733855040.png" alt="1557733855040"></p>
<p>CLR能够发挥作用的一个直观理解是：最小化loss的困难在于如何逃离saddle点而不是在于差的局部最小值。在saddle 点的附近，梯度都很小因此学习的过程缓慢，因此通过增加学习率可以更快地走出saddle点区域。经验上的理由为什么CLR能够work是这样的：最佳的学习率可能在min-max boundaries之间，在最佳的学习率附近会被用于进行训练。（其他会被用于脱离saddle点。。。）。</p>
<p>除了上面显示的trangular learning rate policy,还有以下两种:</p>
<ol>
<li><p>triangular2, 和triangular差不多，差别在于每一个cycle之后lr会减半。</p>
</li>
<li><p>exp_range，boundary的值会以一个指数因子衰减。</p>
</li>
</ol>
<p><img src="/images/assets/1557737380100.png" alt="1557737380100"></p>
<p><img src="/images/assets/1557737358780.png" alt="1557737358780"></p>
<p>里面还讲了如何去估计一个cycle len的方法：</p>
<p>stepsize最好是2-10倍的每个epoch的迭代次数。对于CIFAR10来说，stepsize=8也就比stepsize=2效果好上一点点。</p>
<p>此外还讲了如何估计一个合理的min和max boundary</p>
<p>第一个方法就是：“LR range test”,模型先跑几个epoch，然后让lr从一个很小值增加到很大的值。然后画出accuracy versus learning rate.如下图：</p>
<p><img src="/images/assets/1557735295110.png" alt="1557735295110"></p>
<p>注意图中的accuracy开始增加和accuracy开始变缓的时间段(或者accuracy开始下降)的地方。 这两个地方是bound是的一个好的选择。即base_lr是第一个值，而max_lr是第二个值。或者说可以用一个经验，将base_lr设置为1/3或1/4的max_lr. <strong>论文中作者选了base_lr = 0.001,而max_lr = 0.006</strong></p>
<p>另外一个选择bound是的方法式画出loss versus learning rate的图，如下：</p>
<p><img src="/images/assets/1557735640796.png" alt="1557735640796"></p>
<p>这张图中最适合的lr是哪里？不是在最低点，因为在最低点的lr已经有点大了。我们需要的是一个点更aggressive，所以我们能够train很快。即那个点loss下降是<strong>最快</strong>的</p>
<h2 id="实验过程"><a href="#实验过程" class="headerlink" title="实验过程"></a>实验过程</h2><p>做kaggle比赛的时候，clr的base_lr和max_lr设置反了，特别是在开始的一个stepsize里面，速度非常快，很容易就达到了一个很好的acc ，但是过了这个stepsize，acc就不断下降。一开始举得clclr的问题，后来突然发现是我输入的参数错误。有鉴于它收敛非常快，我觉得还是要借鉴下，发现lr的变化是这样的：</p>
<p><img src="/images/assets/1557732898573.png" alt="1557732898573"></p>
<p>和clr差了一个stepsize。这个和SGDR很相似，准备用这个试试。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://arxiv.org/pdf/1506.01186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01186.pdf</a></p>
<p><a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate" target="_blank" rel="noopener">https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate</a></p>
<p><a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0" target="_blank" rel="noopener">https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0</a></p>
<p><a href="https://www.paperweekly.site/papers/notes/598" target="_blank" rel="noopener">https://www.paperweekly.site/papers/notes/598</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Cyclical-Learning-Rates-for-Training-Neural-Networks/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/TorchVision-Image-Transforms/" >TorchVison Image Transforms</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>transforms主要是图像transform, 它们可以通过使用Compose来链接起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transforms.Compose([</span><br><span class="line">  transforms.CenterCrop(<span class="number">10</span>),</span><br><span class="line">  transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h2 id="Transforms-on-PIL-Image"><a href="#Transforms-on-PIL-Image" class="headerlink" title="Transforms on PIL Image"></a>Transforms on PIL Image</h2><h4 id="torchvision-transforms-CenterCrop-size"><a href="#torchvision-transforms-CenterCrop-size" class="headerlink" title="torchvision.transforms.CenterCrop(size):"></a><code>torchvision.transforms.CenterCrop</code>(<em>size</em>):</h4><p>对给定的PIL image在中心处裁剪。</p>
<p>参数为：<strong>size</strong>, int or sequence. 如果是一个sequence，比如（h,w）会裁剪一个h*w大小的图片。</p>
<p>如果是int，那么会裁剪大小为（size，size）的图像</p>
<h4 id="torchvision-transforms-FiveCrop-size"><a href="#torchvision-transforms-FiveCrop-size" class="headerlink" title="torchvision.transforms.FiveCrop(size)"></a><code>torchvision.transforms.FiveCrop</code>(<em>size</em>)</h4><p>对给定的PIL image的四个角和中心进行裁剪</p>
<p>其他同上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>transform = Compose([</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>   FiveCrop(size), <span class="comment"># this is a list of PIL Images</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>   Lambda(<span class="keyword">lambda</span> crops: torch.stack([ToTensor()(crop) <span class="keyword">for</span> crop <span class="keyword">in</span> crops])) <span class="comment"># returns a 4D tensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>])</span><br></pre></td></tr></table></figure>

<h4 id="torchvision-transforms-Pad-padding-fill-0-padding-mode-’constant’"><a href="#torchvision-transforms-Pad-padding-fill-0-padding-mode-’constant’" class="headerlink" title="torchvision.transforms.Pad(padding, fill=0, padding_mode=’constant’)"></a><code>torchvision.transforms.Pad</code>(<em>padding</em>, <em>fill=0</em>, <em>padding_mode=’constant’</em>)</h4><p>用给定的pad值对图像的4个sides进行填充</p>
<p>参数：padding: 用于确定每个border填充的数量. </p>
<p>​    如果只有一个int，对所有的边进行一样的填充数量</p>
<p>​    如果为长度为2的tuple，那么是对左右，上下分别指定</p>
<p>​    如果长度为4的tuple，那么是对左、上，右、下的边分别指定 </p>
<p>fill: 当mode为constfill时的填充值。默认为0，如果是一个长度为3的tuple是，分别为RGB值</p>
<p>padding_mode:padding的类型</p>
<p>​    constant，常数填充</p>
<p>​    edge：用edge上的值进行填充</p>
<p>​    reflect：pads with reflection of image without repeating the last value on the edge</p>
<p>​    symmeic：pads with reflection of image repeating the last value on the edge</p>
<h4 id="torchvision-transforms-Grayscale-num-output-channels-1"><a href="#torchvision-transforms-Grayscale-num-output-channels-1" class="headerlink" title="torchvision.transforms.Grayscale(num_output_channels=1)"></a><code>torchvision.transforms.Grayscale</code>(<em>num_output_channels=1</em>)</h4><p>将image转为灰度图</p>
<p>参数：<strong>num_output_channels</strong> ，默认为1，也可以为3, 是想要输出图像的channel的个数。</p>
<p>输出：输入的灰度版本。如果nums为1，那么返回的image是单channel，如果是3，返回的image的三个r、g、b三个通道相等。</p>
<p>输出的type：PIL image</p>
<h4 id="torchvision-transforms-Resize-size-interpolation-2"><a href="#torchvision-transforms-Resize-size-interpolation-2" class="headerlink" title="torchvision.transforms.Resize(size, interpolation=2)"></a><code>torchvision.transforms.Resize</code>(<em>size</em>, <em>interpolation=2</em>)</h4><p>将输入的PILimage的大小resize到给定的大小</p>
<p>参数：<strong>size</strong> (<em>sequence</em> <em>or</em> int)期望的输出。如果size是int，那么短的边会匹配到这个数字。ie，如果height&gt;height, 那么image会被缩放为(size*height/width, size). 如果size为sequence，那么大小会被匹配到给定的（h,w）。</p>
<p><strong>interpolation</strong>: 插值的方法，默认为PIL.Image.BILINEAR</p>
<h2 id="Transforms-on-torch-Tensor"><a href="#Transforms-on-torch-Tensor" class="headerlink" title="Transforms on torch.*Tensor"></a>Transforms on torch.*Tensor</h2><h4 id="torchvision-transforms-Normalize-mean-std-inplace-False"><a href="#torchvision-transforms-Normalize-mean-std-inplace-False" class="headerlink" title="torchvision.transforms.Normalize(mean, std, inplace=False)"></a><code>torchvision.transforms.</code>Normalize(<em>mean</em>, <em>std</em>, <em>inplace=False</em>)</h4><p>归一化给定的mean，std来归一化一张tensor image。对于每一个channel进行<br>$$<br>\frac{（input[channel - mean[channel]）}{std[channel]}<br>$$<br>参数：mean：每个channel的均值</p>
<p>std: 每个channel的std值</p>
<p>返回：normalized Tensor image</p>
<p>返回类型：Tensor</p>
<p><strong>Note</strong>：不是就地改变输入Tensor</p>
<h2 id="Conversion-Transforms"><a href="#Conversion-Transforms" class="headerlink" title="Conversion Transforms"></a>Conversion Transforms</h2><h4 id="torchvision-transforms-ToPILImage-mode-None"><a href="#torchvision-transforms-ToPILImage-mode-None" class="headerlink" title="torchvision.transforms.ToPILImage(mode=None)"></a><code>torchvision.transforms.ToPILImage</code>(<em>mode=None</em>)</h4><p>将Tensor或者ndarray转换为PILimage</p>
<p>参数：mode:</p>
<p>如果mode没给定：</p>
<p>​    如果输入为4channel，那么默认为RGBA</p>
<p>​    如果输入为3channel，那么默认为RGB</p>
<p>​    如果输入为2channel，那么默认为LA</p>
<p>​    如果输入为1 channel，那么由mode参数确定</p>
<h4 id="torchvision-transforms-ToTensor"><a href="#torchvision-transforms-ToTensor" class="headerlink" title="torchvision.transforms.ToTensor"></a><code>torchvision.transforms.ToTensor</code></h4><p>将PIL image 或者ndarray转换为Tensor</p>
<p>将值范围为【0，255】的PIL image或者ndarray（H/<em>W/</em>C）转换为FloatTensor(C,H,W)并且值范围为【0.0，1.0】，如果the PIL Image属于 one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) 或者 the numpy.ndarray has dtype = np.uint8</p>
<p>其他的，tensors不会进行缩放</p>
<h4 id="FiveCrop和TenCrop"><a href="#FiveCrop和TenCrop" class="headerlink" title="FiveCrop和TenCrop"></a>FiveCrop和TenCrop</h4><p>这两种操作之后,一张图变成五张,一张图变成十张,那么在训练或者测试的时候怎么避免和标签混淆呢<br>思路是,这多个图拥有相同的标签,假如是分类任务,就可以使用交叉熵进行,然后求10张图的平均</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = Compose([</span><br><span class="line">    TenCrop(size), <span class="comment"># this is a list of PIL Images</span></span><br><span class="line">    Lambda(<span class="keyword">lambda</span> crops: torch.stack([ToTensor()(crop) <span class="keyword">for</span> crop <span class="keyword">in</span> crops])) <span class="comment"># returns a 4D tensor</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#In your test loop you can do the following:</span></span><br><span class="line">input, target = batch <span class="comment"># input is a 5d tensor, target is 2d</span></span><br><span class="line">bs, ncrops, c, h, w = input.size()</span><br><span class="line">result = model(input.view(<span class="number">-1</span>, c, h, w)) <span class="comment"># fuse batch size and ncrops</span></span><br><span class="line"></span><br><span class="line">result_avg = result.view(bs, ncrops, <span class="number">-1</span>).mean(<span class="number">1</span>) <span class="comment"># avg over crops</span></span><br></pre></td></tr></table></figure>


	
	</div>
  <a type="button" href="/2019/07/02/TorchVision-Image-Transforms/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Pytorch加载和读取模型/" >Pytorch加载和读取模型</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>首先看下有关的函数：</p>
<ol>
<li>torch.save: 将一个文件保存到硬盘上，内部是用了pickle库</li>
<li>troch.load：用的pickle的unpicking方法将存储在硬盘上的object读取到内存中</li>
<li>torch.nn.Module.load_state_dict：从一个state_dict中加载一个模型的参数</li>
</ol>
<p>什么是state_dict:</p>
<p>pytorch中的每个module的可学习的参数：如权重和bias等都在module.parameters()里面。</p>
<p>一个state_dict简单来说就是一个字典object，可以把每一层映射到他的参数上去。可学习参数以及register buffer(bn)已经优化器都有state_dict。因为state_dict是python字典对象，因此很简单就可以保存，修改。</p>
<p>下面是读取模型state_dict的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param_tensor <span class="keyword">in</span> model.state_dict():</span><br><span class="line">    print(param_tensor, <span class="string">"\t"</span>, model.state_dict()[param_tensor].size())</span><br></pre></td></tr></table></figure>

<h2 id="两种方法"><a href="#两种方法" class="headerlink" title="两种方法"></a>两种方法</h2><p>回到正题，有两种方法可以保存和读取模型。</p>
<p>第一种是通过模型的state_dict来进行读取和保存。特别是读取的时候，首先得新建一个模型object，然后加载参数。</p>
<p><strong>Save:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p><strong>Load:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>

<p>第二种方法之别保存和加载整个模型：</p>
<p><strong>Save:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br></pre></td></tr></table></figure>

<p><strong>Load:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model class must be defined somewhere</span></span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>

<p>这种方法的缺点是序列化数据绑定到特定类以及保存模型时使用的确切目录结构。这是因为pickle不保存模型类本身。相反，它会保存包含类的文件的路径，该文件在加载时使用。因此，当您在其他项目中或在重构之后使用时，您的代码可能会以各种方式中断。</p>
<h2 id="保存checkpoint"><a href="#保存checkpoint" class="headerlink" title="保存checkpoint"></a>保存checkpoint</h2><p>可以保存checkpoint用于后续的推理和重新训练。和单独保存模型的参数不同，优化器的参数也会被保存，以便于后续的训练。</p>
<h3 id="Save"><a href="#Save" class="headerlink" title="Save:"></a>Save:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">            &apos;epoch&apos;: epoch,</span><br><span class="line">            &apos;model_state_dict&apos;: model.state_dict(),</span><br><span class="line">            &apos;optimizer_state_dict&apos;: optimizer.state_dict(),</span><br><span class="line">            &apos;loss&apos;: loss,</span><br><span class="line">            ...</span><br><span class="line">            &#125;, PATH)</span><br></pre></td></tr></table></figure>

<h3 id="Load"><a href="#Load" class="headerlink" title="Load:"></a>Load:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">'model_state_dict'</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">'optimizer_state_dict'</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">'epoch'</span>]</span><br><span class="line">loss = checkpoint[<span class="string">'loss'</span>]</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line"><span class="comment"># - or -</span></span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/saving_loading_models.html</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Pytorch加载和读取模型/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Normalize的作用/" >Normalize的作用</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>normalize的最主要的一个作用是将数据中的不同的特征缩放到同一个量纲上（或者可以说无量纲化）。比如果说有一个特征值的范围是[0,1]另一个特征的范围是[0,1000],那么优化算法（尤其是基于梯度的优化方法）在更新的时候尤其会重视特征值大的特征，而忽视特征值小的特征。为了避免这个问题就需要normalization了，把所有的特征放在一个量纲上。</p>
<p><img src="/images/assets/743682-20151108152327539-2039269197.png" alt="img"></p>
<h2 id="常用的normalization的方法"><a href="#常用的normalization的方法" class="headerlink" title="常用的normalization的方法"></a>常用的normalization的方法</h2><p>主要有两种方法，min-max normalization 和 Z-score normalization。</p>
<h3 id="min-max-normalization"><a href="#min-max-normalization" class="headerlink" title="min-max normalization"></a>min-max normalization</h3><p>$$<br>x_{minmax} = \frac{x - x_{min}}{x_{max} - x_{min}}<br>$$</p>
<p>主要有两个缺陷:</p>
<ol>
<li>新加入的数据会导致$x_{max}$和$x_{min}$ 会发生变化，需要重新定义</li>
<li>异常值会极大地影响minmax的表现</li>
<li>minmax不适用于长尾分布</li>
</ol>
<p>比较适合于min和max固定的任务，比如图像像素归一化。</p>
<h3 id="Z-score-normalization"><a href="#Z-score-normalization" class="headerlink" title="Z-score normalization"></a>Z-score normalization</h3><p>$$<br>x_{zscore} = \frac{x - min(x)}{stdev(x)}<br>$$</p>
<p>z-score的问题没有min-max多，对异常值也较为鲁棒性。且经过处理的数据会较为贴近正态分布（不是变为），大多数的数据会聚集在0附近，方差为1.</p>
<blockquote>
<p>Caveat:* it is a common misconception that <em>standardized scores</em> such as <em>z-scores</em> alter the shape of a distribution; in particular, be aware that a <em>z</em>-scores cannot magically make a non-normal variable normal.</p>
</blockquote>
<p>其他的还有logistic，lognormal，TanH等，见</p>
<h2 id="Normalizing"><a href="#Normalizing" class="headerlink" title="Normalizing"></a>Normalizing</h2><p>和上面不同的方式，是直接对样本进行单位化，即<br>$$<br>x = \frac{x}{norm(x)}<br>$$<br>不同的norm会有不同的结果，常见的是L2 norm</p>

	
	</div>
  <a type="button" href="/2019/07/02/Normalize的作用/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/PyTorch-weight-decay（转）/" >PyTorch weight decay（转）</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>torch.optim 中实现了很多优化器，只需要指定优化器的权重衰减即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.9</span>,weight_decay=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>

<p>优化器同时还支持per-parameter options操作，就是对每一个参数进行特定的制定，以满足更为细致的要求。此时，传入优化器的是可迭代的字典，字典中必须有params的key，用于指定特定优化变量，而其他key需要匹配优化器本身的设置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">                &#123;&apos;params&apos;: model.base.parameters()&#125;,</span><br><span class="line">                &#123;&apos;params&apos;: model.classifier.parameters(), &apos;lr&apos;: 1e-3&#125;</span><br><span class="line">            ], lr=1e-2, momentum=0.9)</span><br></pre></td></tr></table></figure>

<p>可以灵活给每个子模块设置不同的学习率，权值衰减和momentum。也可以给权值设定权值衰减，而不作用于偏置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">weight_p, bias_p = [],[]</span><br><span class="line">for name, p in model.named_parameters():</span><br><span class="line">  if &apos;bias&apos; in name:</span><br><span class="line">     bias_p += [p]</span><br><span class="line">   else:</span><br><span class="line">     weight_p += [p]</span><br><span class="line"># 这里的model中每个参数的名字都是系统自动命名的，只要是权值都是带有weight，偏置都带有bias，</span><br><span class="line"></span><br><span class="line">optim.SGD([</span><br><span class="line">          &#123;&apos;params&apos;: weight_p, &apos;weight_decay&apos;:1e-5&#125;,</span><br><span class="line">          &#123;&apos;params&apos;: bias_p, &apos;weight_decay&apos;:0&#125;</span><br><span class="line">          ], lr=1e-2, momentum=0.9)</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://blog.csdn.net/LoseInVain/article/details/81708474" target="_blank" rel="noopener">https://blog.csdn.net/LoseInVain/article/details/81708474</a> </p>

	
	</div>
  <a type="button" href="/2019/07/02/PyTorch-weight-decay（转）/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Softmax-and-Logsoftmax-in-Pytorch/" >Softmax and Logsoftmax in Pytorch</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>Output layer and criterion options (all are equivalent, 1 is most popular) :</p>
<ol>
<li>Linear + LogSoftMax + ClassNLLCriterion</li>
<li>Linear + SoftMax + Log + ClassNLLCriterion</li>
<li>Linear + CrossEntropyCriterion</li>
</ol>
<p>It should be noted that <strong>CrossEntropyLoss includes a softmax operation.</strong></p>
<p>softmax with log-likelihood cost can be more fast compared with softmax with MSELoss.</p>
<p>The <strong>log-likelihood loss</strong> is<br>$$<br>C = - \Sigma_k y_klog(a_k)<br>$$<br>where $a_k$ is the output of a neuron, and $y_k$ is the truth.</p>
<p>The <strong>cross-entropy loss</strong> is<br>$$<br>C_{CE} = -\Sigma_k \ y_klog(a_k) + (1-y_k)log(1-a_k)<br>$$<br>And what’s the logsoftmax?<br>$$<br>Applies \ the \ <code>\log(\text{Softmax}(x))</code> function \  to \ an \ n-dimensional<br>    \ input \ Tensor. \ The \ LogSoftmax \ formulation \ can \ be \ simplified \ as:\</p>
<pre><code>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</code></pre><p>$$<br>what’s more, it’s actually realized in nn.functional<br>$$<br>While \ mathematically \ equivalent \  to \   log(softmax(x)), \  doing \  these \   two \ operations \  separately  \ is  \ slower,  \  and  \ numerically  \ unstable.\ This \  function \<br>    \  uses \  an \  alternative \  formulation \  to  \ compute  \ the  \ output \  and  \ gradient  \ correctly.<br>$$<br>The <strong>NLLoss</strong> is:<br>$$<br>Loss \ = - w_nx_{n,y_n}<br>$$<br>where $w_n$ default is 1.</p>
<p>The <strong>BCELoss</strong> is a CrossEntropyLoss designed for binary classification. And it need a sigmoid function before useing the BCELoss. What’s more, <strong>BCEWithLogitsLoss</strong> includes the BCELoss and the sigmoid function.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><p><a href="https://github.com/torch/nn/issues/357" target="_blank" rel="noopener">https://github.com/torch/nn/issues/357</a></p>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#log_softmax" target="_blank" rel="noopener">https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#log_softmax</a></p>
<p><a href="https://pytorch.org/docs/stable/nn.html?highlight=log_softmax#torch.nn.functional.log_softmax" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=log_softmax#torch.nn.functional.log_softmax</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Softmax-and-Logsoftmax-in-Pytorch/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Pytorch-SGDR/" >Pytorch SGDR</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h2 id="SGDR-paper"><a href="#SGDR-paper" class="headerlink" title="SGDR paper"></a>SGDR paper</h2><p>学习率schedule最常见的方法是用一个lr，然后每隔几个epoch除以一个数来减少lr。如下图中的蓝色⚪线和红色</p>
<p>的方块线。</p>
<p><img src="/images/assets/1557729884981.png" alt="1557729884981"></p>
<p>这篇论文所提出的方法是SGD的warm restart版本，即在每次restart，lr都被设置到初始值，但是他的上一次restart到下一次restart之间的距离（schedule）会增加。作者的经验表明，他的这个方法可以比其他的方法快2~4倍达到一个好的效果或者更好的效果。</p>
<p>warm started run SGD T_i 次，其中i是run的index。重要的是，重启不是从头开始执行，而是通过提高学习速率ηt来模拟，而旧的xt值用作初始解决方案</p>
<p>在第i次run，lr decay 是对每个batch用cosine annealing.</p>
<p><img src="E:%5C%E5%88%98%E5%BF%97%E4%BC%9F%E7%9A%84%E6%96%87%E4%BB%B6%5C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5Cpytorch%5Cassets%5C1557730343579.png" alt="1557730343579"><br>$$<br>\eta_{min} 和 \eta_{max}是学习率的范围。 \<br>T_{cur}是距离上次restart所经过的epoch的数量。T_{cur}是每个batch增加，他可以是小数。 \<br>当t=0\ and\ T_{cur} = 0时，T_{cur}=T_{max} \<br>当T_{cur}=T_{max}时， cos 函数会输出-1.因此，\eta_t = \eta_{min}^i</p>
<p>$$<br>图1的绿色线、黑色线和灰色线显示了lr的变化过程。分别固定了$T_i$为50，100，200.</p>
<p>SGDR更进一步选了这么一方法，首先开始的时候$T_i$很小，然后在每次restart都通过乘上一个 $T_{mult}$的因此来提高。例如图一中的暗绿和粉色线。</p>
<h2 id="SGDR-in-pytorch"><a href="#SGDR-in-pytorch" class="headerlink" title="SGDR in pytorch"></a>SGDR in pytorch</h2><p>pytorch只实现了CosineAnnealingLR，并没有实现restart部分。</p>
<p><code>torch.optim.lr_scheduler.CosineAnnealingLR</code>(<em>optimizer</em>, <em>T_max</em>, <em>eta_min=0</em>, <em>last_epoch=-1</em>)</p>
<p><img src="/images/assets/1557731465430.png" alt="1557731465430"></p>
<p>它的用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1.</span>)</span><br><span class="line">steps = <span class="number">10</span></span><br><span class="line">scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(steps):</span><br><span class="line">        scheduler.step()</span><br><span class="line">        print(scheduler.get_lr())</span><br></pre></td></tr></table></figure>

<p>实际上，可以通过下面的方式来实现SGDR</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1.</span>)</span><br><span class="line">steps = <span class="number">10</span></span><br><span class="line">scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(steps):</span><br><span class="line">        scheduler.step()</span><br><span class="line">        print(scheduler.get_lr())</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Reset scheduler'</span>)</span><br><span class="line">    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/18" target="_blank" rel="noopener">https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/18</a></p>
<p><a href="https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR" target="_blank" rel="noopener">https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR</a></p>
<p><a href="https://arxiv.org/pdf/1608.03983.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1608.03983.pdf</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Pytorch-SGDR/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/imaterialist-product-2019比赛总结/" >imaterialist-product-2019比赛总结</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>为了更好地学习pytorch，同时大佬榕也推荐了这个比赛，就拿这个比赛练手。经过2个月左右的奋斗（划水），比赛结束了，成绩是34/96。不是一个很好的成绩，但是也不是一个很差的成绩。这次主要是对这次的比赛做个总结，总结经验，为后面的比赛做一个预备吧。</p>
<p>这次比赛是由三个人小队组成，我，大佬榕和小许。我是队长，写了所有的pytorch代码，负责模型的训练。大佬榕和小许负责了数据集的下载。数据的下载花费了几个星期，同时训练模型也花了一个多月时间。</p>
<h2 id="比赛的内容"><a href="#比赛的内容" class="headerlink" title="比赛的内容"></a>比赛的内容</h2><p>首先讲下这个比赛的内容，是产品的细粒度的图像分类任务。什么是细粒度分类？细粒度图像分类任务中，有一些大类别，每个大类别底下还有小类别。小类别之间的差异比大类别之间的差异更加细微。在imaterialist-product-2019中，主要有4层类别，第一层共有5类，然后第二层有45类，第三层大约有百来类，最后一层则共有2019类。所有的类别的具体名字不知道，只有一个代名字，最后一层则有类别的标号。</p>
<p>具体的类别信息在：</p>
<p>​link <a href="https://storage.googleapis.com/kaggle-competitions-data/kaggle/13773/368464/product_tree.pdf?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&amp;Expires=1562147488&amp;Signature=QUPL60xTxNZW5c4whytfOWYuQfR5eEY0JFIpZE%2F5CE6RQXA6bPia6XpUjOtQRrkw1lhXr7HQiSdBEP1C3X23G4MkBV0CtDY2Uvyzfv3WSVQWUhEt4cbZCj68AoZKr8gtI6%2BFqddv%2FRF%2F%2FWXnbrydhKjjT6mUgg0gSVsSjAKFZJC0mO8SUwbaR9x9%2FJuGX4yxdXSv5Th5Hy%2BFskpXH24fmcbCsfQGRo5Y6hilx3UfaZAYTYZ1b2bIfmk9qvahTWuaYBtP%2F6Ta2Rhig4M3CSFx3dFR7Q79AFH0wBywd1viMRSIiNbprPcTL4spqDxy%2BS3%2FXv1Km93GlsydNBmtjMdfKQ%3D%3D" target="_blank" rel="noopener">https://storage.googleapis.com/kaggle-competitions-data/kaggle/13773/368464/product_tree.pdf?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&amp;Expires=1562147488&amp;Signature=QUPL60xTxNZW5c4whytfOWYuQfR5eEY0JFIpZE%2F5CE6RQXA6bPia6XpUjOtQRrkw1lhXr7HQiSdBEP1C3X23G4MkBV0CtDY2Uvyzfv3WSVQWUhEt4cbZCj68AoZKr8gtI6%2BFqddv%2FRF%2F%2FWXnbrydhKjjT6mUgg0gSVsSjAKFZJC0mO8SUwbaR9x9%2FJuGX4yxdXSv5Th5Hy%2BFskpXH24fmcbCsfQGRo5Y6hilx3UfaZAYTYZ1b2bIfmk9qvahTWuaYBtP%2F6Ta2Rhig4M3CSFx3dFR7Q79AFH0wBywd1viMRSIiNbprPcTL4spqDxy%2BS3%2FXv1Km93GlsydNBmtjMdfKQ%3D%3D</a></p>
<h2 id="数据集的信息"><a href="#数据集的信息" class="headerlink" title="数据集的信息"></a>数据集的信息</h2><p>类别总共是2019类。</p>
<p>训练样本总数是：1,004,360 </p>
<p>验证集样本总数是：10,095</p>
<p>测试集样本总数是：90,834 </p>
<p>其中训练集的图片有缺失，验证集也有。测试集没有。</p>
<p>训练集中，每个类样本数最多的有 1045 ；最少的有： 157 ；</p>
<p>样本个数的分布图为：</p>
<p><img src="/images/assets/1561895928822.png" alt="1561895928822"></p>
<p>对于验证集合，所有的类别个数都为5个。样本个数分布图为：</p>
<p><img src="/images/assets/1561896059345.png" alt="1561896059345"></p>
<p>可以看出训练样本集中各个类的样本数量相差很大，多的上千个，而少的一百来个，样本不均衡这个问题是一个需要解决的问题。</p>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>一台带titan Xp的windows 10 机子，pytorch1.0</p>
<h2 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h2><p>采用的分类模型主要有：resnet（resnet50，resnet101，resnet161），densent(densenet161),其最好的性能列表如下:</p>
<table>
<thead>
<tr>
<th align="center">model</th>
<th align="center">top-1</th>
<th>top-5</th>
</tr>
</thead>
<tbody><tr>
<td align="center">resnet50</td>
<td align="center">59.7</td>
<td>86.25</td>
</tr>
<tr>
<td align="center">resnet101</td>
<td align="center">61.4</td>
<td>87.2</td>
</tr>
<tr>
<td align="center">resnet161</td>
<td align="center">59.5</td>
<td>85.7</td>
</tr>
<tr>
<td align="center">densenet161</td>
<td align="center">-</td>
<td>87.56</td>
</tr>
</tbody></table>
<p>后来使用了model average的方法（其中使用了一些非最佳的模型），大概集成了8个model得到了一个最好的验证精度90.585938</p>
<h2 id="训练过程的细节"><a href="#训练过程的细节" class="headerlink" title="训练过程的细节"></a>训练过程的细节</h2><h5 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h5><p>去除了下载中出现错误的图片</p>
<h5 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h5><p>训练：</p>
<p>resize(224)</p>
<p>randomcrop(224,224)</p>
<p>ColorJitter</p>
<p>RandomHorizontalFlip</p>
<p>RandomRotation</p>
<p>normalize</p>
<p>测试（验证）：</p>
<p>resize（224）</p>
<p>centorcrop（224）</p>
<p>normalize</p>
<h5 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h5><p>用pytorch自带的dataloader，pin_memory=True, num_workers=4,batch_size 根据模型的不同，尽量取大的，并将所有的数据集放在SSD中，加快读取。</p>
<p>没有pin_memory 和ssd大概要4个小时一个epoch（resnet50）。</p>
<p>加了这两个设置，大概1.5个小时一个epoch，加速了很多。</p>
<p>但是一般训练一个模型还是需要2天时间（20epoches）。毕竟xp的显存只有11GB，实际中用的最多只能用10GB。</p>
<p>为了节省显存，采用更节约显存的sgd模型。Adam需要保存很多二阶和二阶的梯度信息，同时也没有比SGD好到那里去，因此采用SGD。</p>
<h5 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h5><p>所有的模型都使用了pretrained model.</p>
<p>所有的模型训练的epochs都小于20.</p>
<p>优化器：SGD with momentum</p>
<p>学习率调度：SGDR 和CyclicLR（由于采用了SGDR和CyclicLR，lr的一个调度的结果也被我用于集成。参考黄高的论文）</p>
<p>学习率区间评估：lr_find+eye see</p>
<h2 id="大佬的比赛方案"><a href="#大佬的比赛方案" class="headerlink" title="大佬的比赛方案"></a>大佬的比赛方案</h2><p>比赛结束了，kaggle上出现了大佬的方案，有以下我没有做到的：</p>
<ol>
<li>采用更好的分类模型SENET</li>
<li>考虑使用autoAugment图像增强</li>
<li>考虑类的不均衡问题</li>
<li>数据集中去除小图片</li>
<li>TTA</li>
</ol>
<p>值得一说的处理不均衡问题，我目前了解有三个方法：</p>
<ol>
<li>上采用和下采样；同时pytorch里面也有一个加权采样器；</li>
<li>将训练集分成两个部分，第一部分是均衡的样本集合，每个类个数相同，该部分先用于训练模型；第二个部分是训练集剩下的部分，用于finetune上面的模型</li>
<li>使用加权loss比如focal loss等。</li>
</ol>

	
	</div>
  <a type="button" href="/2019/07/02/imaterialist-product-2019比赛总结/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Extract-Feature-from-a-Pretrained-Pytorch-Model/" >Extract Feature from a Pretrained Pytorch Model</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision.models.vgg as models</span><br><span class="line">input = torch.rand(1, 3, 5, 5)</span><br><span class="line">vgg16 = models.vgg16(pretrained=True)</span><br><span class="line">output = vgg16.features\[:3\](input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>

<p>其中，vgg16.features[:3]的意思是只选取vgg16网络的前三层，然后input作为输入。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://discuss.pytorch.org/t/extracting-and-using-features-from-a-pretrained-model/20723/11" target="_blank" rel="noopener">https://discuss.pytorch.org/t/extracting-and-using-features-from-a-pretrained-model/20723/11</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Extract-Feature-from-a-Pretrained-Pytorch-Model/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Snapshot-Ensembles/" >Snapshot Ensembles</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>Ensembles work best if the individual models</p>
<p> (1) have low test error and </p>
<p>(2) do not overlap in the set of examples they misclassify. </p>
<p><img src="/images/assets/1558078009871.png" alt="1558078009871"></p>
<h3 id="Snapshot-Ensembling"><a href="#Snapshot-Ensembling" class="headerlink" title="Snapshot Ensembling."></a>Snapshot Ensembling.</h3><p>Figure 2 depicts the training process using cyclic and traditional learning rate schedules. At the end of each training cycle, it is apparent that the <strong>model reaches a local minimum with respect to the training loss</strong>. Thus, before raising the learning rate, we take a “snapshot” of the model weights (indicated as vertical dashed black lines). After training M cycles, we have M model snapshots,$ f_1$ . . .$ f_M$ , each of which will be used in the final ensemble.</p>
<h3 id="Ensembling-at-Test-Time"><a href="#Ensembling-at-Test-Time" class="headerlink" title="Ensembling at Test Time."></a>Ensembling at Test Time.</h3><p>The ensemble prediction at test time is the average of the last m (m ≤ M) model’s softmax outputs. Let x be a test sample and let $ h_i (x)$ be the softmax score of snapshot i. The output of the ensemble is a simple average of the last m models:<br>$$<br>h_{Ensemble} = \frac{1}{m}\Sigma_0^{m-1}\ h_{M-i}(x)<br>$$<br>.We always ensemble the last m models, as these models tend to have the lowest test error.</p>
<h3 id="Ensemble-Size"><a href="#Ensemble-Size" class="headerlink" title="Ensemble Size."></a>Ensemble Size.</h3><p>In some applications, it may be beneficial to vary the size of the ensemble dynamically at test time depending on available resources. Figure 3 displays the performance of DenseNet-40 on the CIFAR-100 dataset as the effective ensemble size, m, is varied. Each ensemble consists of snapshots from later cycles, as these snapshots have received the most training and therefore have likely converged to better minima. Although ensembling more models generally gives better performance, <strong>we observe significant drops in error when the second and third models are added to the ensemble</strong>. In most cases, an ensemble of two models outperforms the baseline model.</p>
<h3 id="Restart-Learning-Rate"><a href="#Restart-Learning-Rate" class="headerlink" title="Restart Learning Rate."></a>Restart Learning Rate.</h3><p>The effect of the restart learning rate can be observed in Figure 3. The left two plots show performance when using a restart learning rate of α0 = 0.1 at the beginning of each cycle, and the right two plots show α0 = 0.2. <strong>In most cases, ensembles with the larger restart learning rate perform better, presumably because the strong perturbation in between cycles increases the diversity of local minima.</strong></p>
<p><img src="/images/assets/1558078640762.png" alt="1558078640762"></p>

	
	</div>
  <a type="button" href="/2019/07/02/Snapshot-Ensembles/#more" class="btn btn-default more">Read More</a>
</div>

	       
	     </div>
	     <div>
	       <center>
	         <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

	       </center>
	     </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			<div class="widget">
    
	    <h4 class="dsq-widget-title">Recent Comments</h4>
		<div id="recent-comments"></div>
		<script type="text/javascript">
		    getRecentCommentsList({
			   type: "github" ? "github" : "github",
			   user: "wzpan",
               repo: "hexo-theme-freemind-blog",
               client_id: "bf7d4ba11877db88543e",
               client_secret: "bff8a6b06b745c0bfcdccbe225623ea8e2a057bb",
			   count: "5" ? "5" : 5,
			   recent_comments_target: "#recent-comments"
			});
		</script>
	
</div>

		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/deep-learning/">deep learning<span>10</span></a></li>
		
			<li><a href="/categories/deeplearning/">deeplearning<span>1</span></a></li>
		
			<li><a href="/categories/machine-learning/">machine learning<span>3</span></a></li>
		
			<li><a href="/categories/编程/">编程<span>2</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/transfer-learning/">transfer learning<span>1</span></a></li>
		
			<li><a href="/tags/ensemble/">ensemble<span>1</span></a></li>
		
			<li><a href="/tags/cnn/">cnn<span>5</span></a></li>
		
			<li><a href="/tags/github/">github<span>1</span></a></li>
		
			<li><a href="/tags/KL散度/">KL散度<span>1</span></a></li>
		
			<li><a href="/tags/pytorch/">pytorch<span>10</span></a></li>
		
			<li><a href="/tags/kaggle/">kaggle<span>2</span></a></li>
		
			<li><a href="/tags/normalize/">normalize<span>1</span></a></li>
		
			<li><a href="/tags/交叉熵/">交叉熵<span>1</span></a></li>
		
			<li><a href="/tags/Pillow/">Pillow<span>1</span></a></li>
		
			<li><a href="/tags/cross-entropy/">cross-entropy<span>1</span></a></li>
		
			<li><a href="/tags/softmax/">softmax<span>1</span></a></li>
		
			<li><a href="/tags/L2-Norm/">L2 Norm<span>1</span></a></li>
		
			<li><a href="/tags/Cyclical-LR/">Cyclical LR<span>1</span></a></li>
		
			<li><a href="/tags/weight-decay/">weight decay<span>1</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2019/07/11/KL散度和交叉熵CE/" ><i class="fa fa-file-o"></i>KL散度和交叉熵CE</a>
      </li>
    
      <li>
        <a href="/2019/07/02/Cyclical-Learning-Rates-for-Training-Neural-Networks/" ><i class="fa fa-file-o"></i>Cyclical Learning Rates for...</a>
      </li>
    
      <li>
        <a href="/2019/07/02/TorchVision-Image-Transforms/" ><i class="fa fa-file-o"></i>TorchVison Image Transforms</a>
      </li>
    
      <li>
        <a href="/2019/07/02/长尾分布特征的处理/" ><i class="fa fa-file-o"></i>长尾分布特征的处理</a>
      </li>
    
      <li>
        <a href="/2019/07/02/Pytorch加载和读取模型/" ><i class="fa fa-file-o"></i>Pytorch加载和读取模型</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/wzpan/freemind/" title="Freemind's Github repository." target="_blank"]);">Freemind</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/wzpan" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.linkedin.com/in/hahack" title="My Linkin account." target="_blank"]);">My LinkedIn</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2019 John Doe
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>




<!-- syntax highlighting -->

  <script>
  marked.setOptions({
    highlight: function (code, lang) {
        return hljs.highlightAuto(code).value;
    }
  });
  function Highlighting(){
    var markdowns = document.getElementsByClassName('markdown');
    for(var i=0;i<markdowns.length;i++){
        if(markdowns[i].innerHTML) markdowns[i].innerHTML =marked(markdowns[i].innerHTML);
    }
  }
  window.addEventListener('DOMContentLoaded', Highlighting, false);
  window.addEventListener('load', Highlighting, false);
  </script>


</body>
</html>