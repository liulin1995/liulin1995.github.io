<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>cnn | LZW&#39; Blog</title>
  <meta name="author" content="John Doe">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="LZW&#39; Blog">

  
    <meta property="og:image" content>
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
    <script src="/js/marked.js"></script>
    <script src="/js/comment.js"></script>
    <script src="/js/timeago.min.js"></script>
    <script src="/js/highlight.min.js"></script>
	<script src="/js/spin.min.js"></script>
  
  <!-- analytics -->
  



</head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">LZW&#39; Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      

<!-- title -->
<div class="page-header page-header-inverse ">
  <h1 class="archive-title-tag title title-inverse ">cnn</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      
         <!-- display as entry -->
	     <div class="mypage">
	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Pytorch-TensorboardX-可视化/" >Pytorch TensorboardX 可视化</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h2 id="安装tensorboard"><a href="#安装tensorboard" class="headerlink" title="安装tensorboard"></a>安装tensorboard</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboardX</span><br><span class="line">pip install tensorflow</span><br></pre></td></tr></table></figure>

<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h4 id="引入并创建一个SummaryWriter"><a href="#引入并创建一个SummaryWriter" class="headerlink" title="引入并创建一个SummaryWriter"></a>引入并创建一个SummaryWriter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line">writer = SummaryWriter(&apos;./runs/dogcat1&apos;)  //log_dir is ./run/dogcat</span><br><span class="line">//need close</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>logdir参数要是不指定的话，会自动在生成run文件夹。另外还有一个comment参数，用于指定文件名称。</p>
<h4 id="画loss曲线："><a href="#画loss曲线：" class="headerlink" title="画loss曲线："></a>画loss曲线：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer.add_scalar(&apos;loss&apos;,loss, epoch)</span><br></pre></td></tr></table></figure>

<p>第一个参数为保存参数的名称，第二个参数为Y轴的值，第三个参数为X轴的值</p>
<p>运行该代码后，在log_dir下运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir log_dir  //log_dir 为具体的文件夹</span><br></pre></td></tr></table></figure>

<p>具体为：</p>
<p><img src="/images/assets/1555679643005.png" alt="1555679643005"></p>
<p>结果为：</p>
<p><img src="/images/assets/1555681461022.png" alt="1555681461022"></p>
<h4 id="画激活情况"><a href="#画激活情况" class="headerlink" title="画激活情况"></a>画激活情况</h4><p>用于检查深层网络里面的层激活和权值分布情况，避免梯度消失等。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for name, param in net.named_parameters():</span><br><span class="line">	writer.add_histogram(</span><br><span class="line">		name, param.cpu().clone().data.numpy(), epoch_index)</span><br></pre></td></tr></table></figure>

<p>需要注意的是，如果tensor在gpu需要将其转换到cpu中。</p>
<p>结果为：</p>
<p><img src="/images/assets/1555682223014.png" alt="1555682223014"></p>
<h4 id="画网络结构图"><a href="#画网络结构图" class="headerlink" title="画网络结构图"></a>画网络结构图</h4><p>首先先对某个model进行实例化，如net。然后定义一个输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input = torch.rand(dim1,dim2,dim3,dim4)</span><br><span class="line"></span><br><span class="line">net = LeNet()</span><br><span class="line">writer.add_graph(net, input)</span><br></pre></td></tr></table></figure>

<p>同样的，需要注意net要在cpu中。</p>
<p>效果如下：</p>
<p><img src="/images/assets/1555682403877.png" alt="1555682403877"></p>
<h4 id="显示图片"><a href="#显示图片" class="headerlink" title="显示图片"></a>显示图片</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer.add_image(&apos;name&apos;,image_object)</span><br></pre></td></tr></table></figure>

<h4 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h4><p>使用PCA，T-SNE等方法将高位向量投影到三维坐标系。默认使用PCA，也可以选择T-SNE</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer.add_embedding(mat, metadata=None, label_img=None, global_step=None, tag=&apos;default&apos;, metadata_header=None）</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>mat</strong> (<a href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>torch.Tensor</em></a> <em>or</em> <em>numpy.array</em>) – A matrix which each row is the feature vector of the data point</li>
<li><strong>metadata</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" target="_blank" rel="noopener"><em>list</em></a>) – A list of labels, each element will be convert to string</li>
<li><strong>label_img</strong> (<a href="https://pytorch.org/docs/master/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>torch.Tensor</em></a>) – Images correspond to each data point</li>
<li><strong>global_step</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) – Global step value to record</li>
<li><strong>tag</strong> (<em>string</em>) – Name for the embedding</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="https://tensorboardx.readthedocs.io/en/latest/tensorboard.html" target="_blank" rel="noopener">https://tensorboardx.readthedocs.io/en/latest/tensorboard.html</a></p>
<p><a href="http://www.pianshen.com/article/3479170564/" target="_blank" rel="noopener">http://www.pianshen.com/article/3479170564/</a></p>
<p><a href="https://github.com/pytorch/pytorch/issues/2731" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/2731</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Pytorch-TensorboardX-可视化/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Softmax-and-Logsoftmax-in-Pytorch/" >Softmax and Logsoftmax in Pytorch</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>Output layer and criterion options (all are equivalent, 1 is most popular) :</p>
<ol>
<li>Linear + LogSoftMax + ClassNLLCriterion</li>
<li>Linear + SoftMax + Log + ClassNLLCriterion</li>
<li>Linear + CrossEntropyCriterion</li>
</ol>
<p>It should be noted that <strong>CrossEntropyLoss includes a softmax operation.</strong></p>
<p>softmax with log-likelihood cost can be more fast compared with softmax with MSELoss.</p>
<p>The <strong>log-likelihood loss</strong> is<br>$$<br>C = - \Sigma_k y_klog(a_k)<br>$$<br>where $a_k$ is the output of a neuron, and $y_k$ is the truth.</p>
<p>The <strong>cross-entropy loss</strong> is<br>$$<br>C_{CE} = -\Sigma_k \ y_klog(a_k) + (1-y_k)log(1-a_k)<br>$$<br>And what’s the logsoftmax?<br>$$<br>Applies \ the \ <code>\log(\text{Softmax}(x))</code> function \  to \ an \ n-dimensional<br>    \ input \ Tensor. \ The \ LogSoftmax \ formulation \ can \ be \ simplified \ as:\</p>
<pre><code>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</code></pre><p>$$<br>what’s more, it’s actually realized in nn.functional<br>$$<br>While \ mathematically \ equivalent \  to \   log(softmax(x)), \  doing \  these \   two \ operations \  separately  \ is  \ slower,  \  and  \ numerically  \ unstable.\ This \  function \<br>    \  uses \  an \  alternative \  formulation \  to  \ compute  \ the  \ output \  and  \ gradient  \ correctly.<br>$$<br>The <strong>NLLoss</strong> is:<br>$$<br>Loss \ = - w_nx_{n,y_n}<br>$$<br>where $w_n$ default is 1.</p>
<p>The <strong>BCELoss</strong> is a CrossEntropyLoss designed for binary classification. And it need a sigmoid function before useing the BCELoss. What’s more, <strong>BCEWithLogitsLoss</strong> includes the BCELoss and the sigmoid function.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><p><a href="https://github.com/torch/nn/issues/357" target="_blank" rel="noopener">https://github.com/torch/nn/issues/357</a></p>
<p><a href="https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#log_softmax" target="_blank" rel="noopener">https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#log_softmax</a></p>
<p><a href="https://pytorch.org/docs/stable/nn.html?highlight=log_softmax#torch.nn.functional.log_softmax" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=log_softmax#torch.nn.functional.log_softmax</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Softmax-and-Logsoftmax-in-Pytorch/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Pytorch-SGDR/" >Pytorch SGDR</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h2 id="SGDR-paper"><a href="#SGDR-paper" class="headerlink" title="SGDR paper"></a>SGDR paper</h2><p>学习率schedule最常见的方法是用一个lr，然后每隔几个epoch除以一个数来减少lr。如下图中的蓝色⚪线和红色</p>
<p>的方块线。</p>
<p><img src="/images/assets/1557729884981.png" alt="1557729884981"></p>
<p>这篇论文所提出的方法是SGD的warm restart版本，即在每次restart，lr都被设置到初始值，但是他的上一次restart到下一次restart之间的距离（schedule）会增加。作者的经验表明，他的这个方法可以比其他的方法快2~4倍达到一个好的效果或者更好的效果。</p>
<p>warm started run SGD T_i 次，其中i是run的index。重要的是，重启不是从头开始执行，而是通过提高学习速率ηt来模拟，而旧的xt值用作初始解决方案</p>
<p>在第i次run，lr decay 是对每个batch用cosine annealing.</p>
<p><img src="E:%5C%E5%88%98%E5%BF%97%E4%BC%9F%E7%9A%84%E6%96%87%E4%BB%B6%5C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%5Cpytorch%5Cassets%5C1557730343579.png" alt="1557730343579"><br>$$<br>\eta_{min} 和 \eta_{max}是学习率的范围。 \<br>T_{cur}是距离上次restart所经过的epoch的数量。T_{cur}是每个batch增加，他可以是小数。 \<br>当t=0\ and\ T_{cur} = 0时，T_{cur}=T_{max} \<br>当T_{cur}=T_{max}时， cos 函数会输出-1.因此，\eta_t = \eta_{min}^i</p>
<p>$$<br>图1的绿色线、黑色线和灰色线显示了lr的变化过程。分别固定了$T_i$为50，100，200.</p>
<p>SGDR更进一步选了这么一方法，首先开始的时候$T_i$很小，然后在每次restart都通过乘上一个 $T_{mult}$的因此来提高。例如图一中的暗绿和粉色线。</p>
<h2 id="SGDR-in-pytorch"><a href="#SGDR-in-pytorch" class="headerlink" title="SGDR in pytorch"></a>SGDR in pytorch</h2><p>pytorch只实现了CosineAnnealingLR，并没有实现restart部分。</p>
<p><code>torch.optim.lr_scheduler.CosineAnnealingLR</code>(<em>optimizer</em>, <em>T_max</em>, <em>eta_min=0</em>, <em>last_epoch=-1</em>)</p>
<p><img src="/images/assets/1557731465430.png" alt="1557731465430"></p>
<p>它的用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1.</span>)</span><br><span class="line">steps = <span class="number">10</span></span><br><span class="line">scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(steps):</span><br><span class="line">        scheduler.step()</span><br><span class="line">        print(scheduler.get_lr())</span><br></pre></td></tr></table></figure>

<p>实际上，可以通过下面的方式来实现SGDR</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1.</span>)</span><br><span class="line">steps = <span class="number">10</span></span><br><span class="line">scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(steps):</span><br><span class="line">        scheduler.step()</span><br><span class="line">        print(scheduler.get_lr())</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Reset scheduler'</span>)</span><br><span class="line">    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/18" target="_blank" rel="noopener">https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/18</a></p>
<p><a href="https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR" target="_blank" rel="noopener">https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR</a></p>
<p><a href="https://arxiv.org/pdf/1608.03983.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1608.03983.pdf</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Pytorch-SGDR/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Extract-Feature-from-a-Pretrained-Pytorch-Model/" >Extract Feature from a Pretrained Pytorch Model</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision.models.vgg as models</span><br><span class="line">input = torch.rand(1, 3, 5, 5)</span><br><span class="line">vgg16 = models.vgg16(pretrained=True)</span><br><span class="line">output = vgg16.features\[:3\](input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>

<p>其中，vgg16.features[:3]的意思是只选取vgg16网络的前三层，然后input作为输入。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://discuss.pytorch.org/t/extracting-and-using-features-from-a-pretrained-model/20723/11" target="_blank" rel="noopener">https://discuss.pytorch.org/t/extracting-and-using-features-from-a-pretrained-model/20723/11</a></p>

	
	</div>
  <a type="button" href="/2019/07/02/Extract-Feature-from-a-Pretrained-Pytorch-Model/#more" class="btn btn-default more">Read More</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2019-07-02 </div>
			<div class="article-title"><a href="/2019/07/02/Snapshot-Ensembles/" >Snapshot Ensembles</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<p>Ensembles work best if the individual models</p>
<p> (1) have low test error and </p>
<p>(2) do not overlap in the set of examples they misclassify. </p>
<p><img src="/images/assets/1558078009871.png" alt="1558078009871"></p>
<h3 id="Snapshot-Ensembling"><a href="#Snapshot-Ensembling" class="headerlink" title="Snapshot Ensembling."></a>Snapshot Ensembling.</h3><p>Figure 2 depicts the training process using cyclic and traditional learning rate schedules. At the end of each training cycle, it is apparent that the <strong>model reaches a local minimum with respect to the training loss</strong>. Thus, before raising the learning rate, we take a “snapshot” of the model weights (indicated as vertical dashed black lines). After training M cycles, we have M model snapshots,$ f_1$ . . .$ f_M$ , each of which will be used in the final ensemble.</p>
<h3 id="Ensembling-at-Test-Time"><a href="#Ensembling-at-Test-Time" class="headerlink" title="Ensembling at Test Time."></a>Ensembling at Test Time.</h3><p>The ensemble prediction at test time is the average of the last m (m ≤ M) model’s softmax outputs. Let x be a test sample and let $ h_i (x)$ be the softmax score of snapshot i. The output of the ensemble is a simple average of the last m models:<br>$$<br>h_{Ensemble} = \frac{1}{m}\Sigma_0^{m-1}\ h_{M-i}(x)<br>$$<br>.We always ensemble the last m models, as these models tend to have the lowest test error.</p>
<h3 id="Ensemble-Size"><a href="#Ensemble-Size" class="headerlink" title="Ensemble Size."></a>Ensemble Size.</h3><p>In some applications, it may be beneficial to vary the size of the ensemble dynamically at test time depending on available resources. Figure 3 displays the performance of DenseNet-40 on the CIFAR-100 dataset as the effective ensemble size, m, is varied. Each ensemble consists of snapshots from later cycles, as these snapshots have received the most training and therefore have likely converged to better minima. Although ensembling more models generally gives better performance, <strong>we observe significant drops in error when the second and third models are added to the ensemble</strong>. In most cases, an ensemble of two models outperforms the baseline model.</p>
<h3 id="Restart-Learning-Rate"><a href="#Restart-Learning-Rate" class="headerlink" title="Restart Learning Rate."></a>Restart Learning Rate.</h3><p>The effect of the restart learning rate can be observed in Figure 3. The left two plots show performance when using a restart learning rate of α0 = 0.1 at the beginning of each cycle, and the right two plots show α0 = 0.2. <strong>In most cases, ensembles with the larger restart learning rate perform better, presumably because the strong perturbation in between cycles increases the diversity of local minima.</strong></p>
<p><img src="/images/assets/1558078640762.png" alt="1558078640762"></p>

	
	</div>
  <a type="button" href="/2019/07/02/Snapshot-Ensembles/#more" class="btn btn-default more">Read More</a>
</div>

	       
	     </div>
	     <div>
	       <center>
	         <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

	       </center>
	     </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			<div class="widget">
    
	    <h4 class="dsq-widget-title">Recent Comments</h4>
		<div id="recent-comments"></div>
		<script type="text/javascript">
		    getRecentCommentsList({
			   type: "github" ? "github" : "github",
			   user: "wzpan",
               repo: "hexo-theme-freemind-blog",
               client_id: "bf7d4ba11877db88543e",
               client_secret: "bff8a6b06b745c0bfcdccbe225623ea8e2a057bb",
			   count: "5" ? "5" : 5,
			   recent_comments_target: "#recent-comments"
			});
		</script>
	
</div>

		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/deep-learning/">deep learning<span>10</span></a></li>
		
			<li><a href="/categories/deeplearning/">deeplearning<span>1</span></a></li>
		
			<li><a href="/categories/machine-learning/">machine learning<span>3</span></a></li>
		
			<li><a href="/categories/编程/">编程<span>2</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/交叉熵/">交叉熵<span>1</span></a></li>
		
			<li><a href="/tags/Pillow/">Pillow<span>1</span></a></li>
		
			<li><a href="/tags/weight-decay/">weight decay<span>1</span></a></li>
		
			<li><a href="/tags/KL散度/">KL散度<span>1</span></a></li>
		
			<li><a href="/tags/ensemble/">ensemble<span>1</span></a></li>
		
			<li><a href="/tags/kaggle/">kaggle<span>2</span></a></li>
		
			<li><a href="/tags/pytorch/">pytorch<span>10</span></a></li>
		
			<li><a href="/tags/github/">github<span>1</span></a></li>
		
			<li><a href="/tags/L2-Norm/">L2 Norm<span>1</span></a></li>
		
			<li><a href="/tags/normalize/">normalize<span>1</span></a></li>
		
			<li><a href="/tags/Cyclical-LR/">Cyclical LR<span>1</span></a></li>
		
			<li><a href="/tags/cross-entropy/">cross-entropy<span>1</span></a></li>
		
			<li><a href="/tags/softmax/">softmax<span>1</span></a></li>
		
			<li><a href="/tags/transfer-learning/">transfer learning<span>1</span></a></li>
		
			<li><a href="/tags/cnn/">cnn<span>5</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2019/07/11/KL散度和交叉熵CE/" ><i class="fa fa-file-o"></i>KL散度和交叉熵CE</a>
      </li>
    
      <li>
        <a href="/2019/07/02/Cyclical-Learning-Rates-for-Training-Neural-Networks/" ><i class="fa fa-file-o"></i>Cyclical Learning Rates for...</a>
      </li>
    
      <li>
        <a href="/2019/07/02/TorchVision-Image-Transforms/" ><i class="fa fa-file-o"></i>TorchVison Image Transforms</a>
      </li>
    
      <li>
        <a href="/2019/07/02/长尾分布特征的处理/" ><i class="fa fa-file-o"></i>长尾分布特征的处理</a>
      </li>
    
      <li>
        <a href="/2019/07/02/Pytorch加载和读取模型/" ><i class="fa fa-file-o"></i>Pytorch加载和读取模型</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/wzpan/freemind/" title="Freemind's Github repository." target="_blank"]);">Freemind</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/wzpan" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.linkedin.com/in/hahack" title="My Linkin account." target="_blank"]);">My LinkedIn</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2019 John Doe
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>




<!-- syntax highlighting -->

  <script>
  marked.setOptions({
    highlight: function (code, lang) {
        return hljs.highlightAuto(code).value;
    }
  });
  function Highlighting(){
    var markdowns = document.getElementsByClassName('markdown');
    for(var i=0;i<markdowns.length;i++){
        if(markdowns[i].innerHTML) markdowns[i].innerHTML =marked(markdowns[i].innerHTML);
    }
  }
  window.addEventListener('DOMContentLoaded', Highlighting, false);
  window.addEventListener('load', Highlighting, false);
  </script>


</body>
</html>